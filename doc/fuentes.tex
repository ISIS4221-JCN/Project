\section{Selección y explotación de fuentes}

\subsection{Twitter}
Para descargar tweets se utilizó la API oficial de la red social. Para ello fue necesario pedir acceso como desarrollador, y adicionalmente categoría de 'Academic Research' para poder crear un \textit{streamer} que continuamente esté descargando los archivos de lared social.\\ 

Como medida preventiva a que no se obtenga la categoría de investigación académica se procede con la creación de un \textit{script} que recupere todos los Tweets en tiempo real que cumplen con una serie de normas definidas por el usuario para filtrar la información. Este \textit{script} permite descargar información con la categoría estándar. Se crea adicionalmente un archivo que realice búsqueda en los archivos de la red social y retorne aquellos que cumplen con las normas de búsqueda. Este archivo sí requiere tener los permisos de categoría de investigación académica.

\subsection{Tumblr}
Al igual que en el caso de Twitter se utiliza la API oficial de la red social para la descarga de los archivos.\\

Cada solicitud que se hace a Tumblr recupera archivos bajo un tag específico. Lastimosamente, hay un límite de 20 archivos por cada solicitud y 432.000 llamados por día. Además de que no permite hacer una búsqueda organizada sino que en muchas ocasiones retorna una misma respuesta a pesar de ser una nueva solicitud. Para lidiar con este problema, se crea una lista de IDs leídos para así descartar aquellos que estén ya almacenados.\\ 

Adicionalmente es necesario utilizar una librería de detección de lenguaje dado que la información que provee la solicitud no la entrega. Se utiliza \textit{langdetect} de Python para poder almacenar los archivos acorde a como e explicó en la sección correspondiente.