{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid_QA_System.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdNqAh6NEIB6"
      },
      "source": [
        "# COVID Q&A System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESqoeGKXENSK"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Load libraries and document corpus to use as context for the Q&A System"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqWoHMktEM0S"
      },
      "source": [
        "# Install libraries\n",
        "!pip install transformers\n",
        "# For french\n",
        "#!pip install --no-cache-dir transformers sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj_lSWyM-6I9",
        "outputId": "de74e6d9-9f22-4507-b8f0-08ecbb994206"
      },
      "source": [
        "%cd drive/MyDrive/NLP/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "609oXVgDEAYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123062c2-89b4-4636-8ba5-620d9430af0e"
      },
      "source": [
        "# Import required libraries for excecution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# NLP\n",
        "from gensim import corpora, models, similarities\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Import utils class\n",
        "#sys.path.insert(0,'../')\n",
        "from utils import Utils\n",
        "\n",
        "# Transformer Models\n",
        "from transformers import pipeline\n",
        "\n",
        "# Display\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9AdD73Qy6h3",
        "outputId": "0423644e-df7c-436a-e259-5120200a84dd"
      },
      "source": [
        "# Path to directory of docs\n",
        "path_prefix = './'\n",
        "source = 'WHO'\n",
        "lang = 'es'\n",
        "\n",
        "path = os.path.join(path_prefix, source, lang)\n",
        "files_list = os.listdir(path)\n",
        "\n",
        "# Load list of documents\n",
        "doc_list = []\n",
        "doc_title = []\n",
        "for file in files_list:\n",
        "    # Open docs\n",
        "    with open(os.path.join(path, file), 'r+') as file_str:\n",
        "        data_dict = json.load(file_str)\n",
        "        # Append title and text\n",
        "        for i, txt in enumerate(data_dict['text']):\n",
        "            doc_list.append(f\"{data_dict['title'][i]} {txt}\")\n",
        "            doc_title.append(f\"{data_dict['title'][i]}\")\n",
        "\n",
        "print(f\"Se cargaron {len(doc_list)} documentos\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Se cargaron 123 documentos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mcn-YMjrTDi"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xs212v4rSGQ"
      },
      "source": [
        "# Stop Words\n",
        "stop_words = stopwords.words('spanish')\n",
        "\n",
        "# Stemmers\n",
        "stem = SnowballStemmer('spanish')\n",
        "#p_stem = PorterStemmer()\n",
        "\n",
        "# Tokenizers\n",
        "#tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "tk = nltk.RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Lemmatizer\n",
        "lemma = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# Instanciate utils class\n",
        "utils = Utils(path_prefix, num_workers=10)\n",
        "\n",
        "# Create vocab (dictionary)\n",
        "doc_dict = []\n",
        "for doc in doc_list:\n",
        "    doc_dict.append(utils.preprocessing(text=doc, stop_words = stop_words,\n",
        "                                             stemmer = None,\n",
        "                                             tokenizer = tk,\n",
        "                                             lemmatizer = lemma))\n",
        "# Get dict\n",
        "dictionary = corpora.Dictionary(doc_dict)\n",
        "\n",
        "# Create doc corpus\n",
        "doc_corpus = []\n",
        "for doc in doc_list:\n",
        "    doc_corpus.append(dictionary.doc2bow(utils.preprocessing(text=doc, stop_words = stop_words,\n",
        "                                                                  stemmer = None,\n",
        "                                                                  tokenizer = tk,\n",
        "                                                                  lemmatizer = lemma)))\n",
        "    \n",
        "# Create title corpus\n",
        "title_corpus = []\n",
        "for doc in doc_title:\n",
        "    title_corpus.append(dictionary.doc2bow(utils.preprocessing(text=doc, stop_words = stop_words,\n",
        "                                                                  stemmer = None,\n",
        "                                                                  tokenizer = tk,\n",
        "                                                                  lemmatizer = lemma)))\n",
        "\n",
        "# Serializes and saves dictionary and corpus files\n",
        "#dictionary.save('vocab.dict')\n",
        "#corpora.MmCorpus.serialize(\"covid_qa_corpus.mm\", doc_corpus)\n",
        "#corpora.MmCorpus.serialize(\"covid_qa_title.mm\", title_corpus)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKWH-As2shGF"
      },
      "source": [
        "# Load vocabulary, doc_corpus, query_corpus and df with tags\n",
        "#vocabulary = corpora.Dictionary.load('vocab.dict')\n",
        "#doc_corpus = corpora.MmCorpus(\"covid_qa_corpus.mm\")"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHXHwu_3y6z9"
      },
      "source": [
        "## Information Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUobIgF_2Y81"
      },
      "source": [
        "### Create Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI-jePThcUka",
        "outputId": "7e5057da-6ed7-4e25-a548-a73d893f38bb"
      },
      "source": [
        "# Create tfidf model for document corpus\n",
        "tfidf = models.TfidfModel(doc_corpus)\n",
        "tfidf_title = models.TfidfModel(title_corpus)\n",
        "\n",
        "# Model transformation\n",
        "print('Doc Example (tfidf form): ')\n",
        "print(tfidf[doc_corpus][0]) \n",
        "\n",
        "# Similarity Matrix\n",
        "index = similarities.MatrixSimilarity(tfidf[doc_corpus])\n",
        "index_title = similarities.MatrixSimilarity(tfidf_title[title_corpus])\n",
        "\n",
        "# Save index\n",
        "#index.save('similarity_matrix.index')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Doc Example (tfidf form): \n",
            "[(0, 0.15230563029421557), (1, 0.17793545924244475), (2, 0.09669082367676309), (3, 0.09669082367676309), (4, 0.09669082367676309), (5, 0.15230563029421557), (6, 0.11168331251137473), (7, 0.13731314145960394), (8, 0.15230563029421557), (9, 0.13731314145960394), (10, 0.07541614344952798), (11, 0.08927081859867517), (12, 0.15230563029421557), (13, 0.15230563029421557), (14, 0.06906180368840908), (15, 0.08927081859867517), (16, 0.15230563029421557), (17, 0.17210696712629106), (18, 0.15230563029421557), (19, 0.0536884108776402), (20, 0.09669082367676309), (21, 0.11842483934039395), (22, 0.13731314145960394), (23, 0.10598343279313906), (24, 0.048648500815834335), (25, 0.0830938222332551), (26, 0.08927081859867517), (27, 0.10598343279313906), (28, 0.17854163719735033), (29, 0.0830938222332551), (30, 0.18559002078432957), (31, 0.10104597239775717), (32, 0.17793545924244475), (33, 0.07317448578895563), (34, 0.07541614344952798), (35, 0.1421219894570678), (36, 0.20209194479551434), (37, 0.12667580134598636), (38, 0.13731314145960394), (39, 0.04343197474017989), (40, 0.15230563029421557), (41, 0.06199734007198644), (42, 0.10104597239775717), (43, 0.06536111501029822), (44, 0.11842483934039395), (45, 0.08927081859867517), (46, 0.15230563029421557), (47, 0.06716518144393557), (48, 0.15230563029421557), (49, 0.13731314145960394), (50, 0.15230563029421557), (51, 0.02593167574383173), (52, 0.12667580134598636), (53, 0.10598343279313906), (54, 0.13731314145960394), (55, 0.07317448578895563), (56, 0.10598343279313906), (57, 0.09279501039216478), (58, 0.18559002078432957), (59, 0.044418060567597874), (60, 0.12667580134598636), (61, 0.06364098965044597), (62, 0.12667580134598636), (63, 0.06199734007198644), (64, 0.17793545924244475), (65, 0.07541614344952798), (66, 0.09729700163166867), (67, 0.10598343279313906)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhCHywEE2cAO"
      },
      "source": [
        "### Querying"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5Mn0w3M2Me_"
      },
      "source": [
        "def perform_query(query, top_n = -1):\n",
        "\n",
        "    # Preprocess query\n",
        "    processed_query = utils.preprocessing(query, \n",
        "                                          stop_words = stop_words,\n",
        "                                          stemmer = None,\n",
        "                                          tokenizer = tk,\n",
        "                                          lemmatizer = lemma)\n",
        "\n",
        "    # Similarity between all docs and query\n",
        "    #sims = list(enumerate(index[tfidf[dictionary.doc2bow(processed_query)]]))\n",
        "\n",
        "    # Similarity between all doc titles and query\n",
        "    sims = list(enumerate(index_title[tfidf_title[dictionary.doc2bow(processed_query)]]))\n",
        "    \n",
        "    dtype = [('doc_id', int), ('score', float)]\n",
        "    doc_sims = np.array(sims, dtype=dtype)\n",
        "    \n",
        "    # Sort Docs by similarity\n",
        "    doc_sims_sorted = np.flip(np.sort(doc_sims, order='score'))\n",
        "\n",
        "    # Retrieve only documents with non zero score\n",
        "    k = len(np.nonzero(doc_sims['score'])[0])\n",
        "    relevant_docs = doc_sims_sorted[0:k]\n",
        "    \n",
        "    # Print only top docs\n",
        "    context_doc_ids = relevant_docs['doc_id'][0:top_n]\n",
        "    \n",
        "    return context_doc_ids"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFcvo0mJF7wd"
      },
      "source": [
        "## Deep Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSvbU4gCGLBj"
      },
      "source": [
        "# Load pre-trained Q&A Model for lang\n",
        "if lang == 'en':\n",
        "    covid_qa = pipeline(\"question-answering\", model='deepset/roberta-base-squad2-covid')\n",
        "elif lang == 'es':\n",
        "    covid_qa = pipeline(\"question-answering\", model='mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es')\n",
        "elif lang == 'fr':\n",
        "    covid_qa = pipeline('question-answering', model='fmikaelian/camembert-base-fquad', tokenizer='fmikaelian/camembert-base-fquad', use_fast = False)\n",
        "else:\n",
        "    print('Not supported language')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faIZjbVFF7_N"
      },
      "source": [
        "def perform_qa(query, context_ids, confidence_th = 0.01):\n",
        "\n",
        "    # Get context from retrieved docs\n",
        "    for k, id in enumerate(context_ids):\n",
        "        # Get context doc\n",
        "        context = doc_list[id]\n",
        "\n",
        "        # Perform QA\n",
        "        result = covid_qa(question=query, context=context)\n",
        "\n",
        "        # If score is good enough return result\n",
        "        if result['score'] > confidence_th:\n",
        "            return result, k\n",
        "    \n",
        "    # Default response if not good enough answer was found\n",
        "    if lang == 'en':\n",
        "        result = {'answer': 'Sorry, the answer to that question was not found', 'score': 0}\n",
        "    elif lang == 'es':\n",
        "        result = {'answer': 'Disculpe, la respuesta a esa pregunta no se encontró', 'score': 0}\n",
        "    elif lang == 'fr':\n",
        "        result = {'answer': 'Pardon, la réponse à cette question n\\'a pas été trouvée', 'score': 0}\n",
        "    else:\n",
        "        print('Not supported language')\n",
        "\n",
        "    return result, 0\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhQyxcOk3vL0"
      },
      "source": [
        "## Q&A Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC-7xCO2tW3e"
      },
      "source": [
        "# Example questions \n",
        "q_en = [\"What is COVID-19?\",\n",
        "        \"How does COVID-19 spreads?\",\n",
        "        \"What symptoms does covid-19 causes?\", \n",
        "        \"Which kind of mask should I use to protect me from covid?\",\n",
        "        \"How should I protect from covid?\",\n",
        "        \"Should I get a vaccine for covid-19?\"]\n",
        "\n",
        "q_es = [\"¿Qué es el COVID-19?\",\n",
        "        \"¿Cómo se transmite el covid-19?\",\n",
        "        \"¿Cuales son los sintomas del covid-19?\", \n",
        "        \"¿Qué tipo de máscara debería utilizar para protegerme del covid?\",\n",
        "        \"¿Cómo protegerme del covid?\",\n",
        "        \"¿Debería vacunarme para si ya tuve covid-19?\"]\n",
        "\n",
        "q_fr = [\"Qu'est-ce que COVID-19?\",\n",
        "        \"Comment le COVID-19 se propage-t-il?\",\n",
        "        \"Quels symptômes le covid-19 provoque-t-il?\",\n",
        "        \"Quel type de masque dois-je utiliser pour me protéger du covid?\",\n",
        "        \"Comment dois-je me protéger de Covid?\",\n",
        "        \"Dois-je me faire vacciner contre le covid-19?\"]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmAL9QyX32bm",
        "outputId": "d03a5bae-c413-4e7d-dff9-a8f8c00821da"
      },
      "source": [
        "# Select one random question\n",
        "q = q_es[1]\n",
        "\n",
        "# Input question\n",
        "#q = input('COVID Q&A: ')\n",
        "\n",
        "print(f\"Q: {q}\")\n",
        "print('')\n",
        "\n",
        "# Information Retrieval (get context)\n",
        "context_ids = perform_query(q)\n",
        "\n",
        "# Question Answering\n",
        "result, k = perform_qa(q, context_ids, confidence_th = 0.1)\n",
        "\n",
        "# Print results\n",
        "print(f\"Context Docs (Ranked): \")\n",
        "for i, id in enumerate(context_ids):\n",
        "    print(f\"R{i}: {doc_list[id]}\")\n",
        "    if i >= k:\n",
        "      break\n",
        "print('')\n",
        "\n",
        "print(f\"A: {result['answer']}\")\n",
        "print(f\"Score: {result['score']}\")\n",
        "print(f\"Context Doc used: R{k}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: ¿Cómo se transmite el covid-19?\n",
            "\n",
            "Context Docs (Ranked): \n",
            "R0: ¿Qué es la COVID-19? La COVID-19 es la enfermedad causada por el nuevo coronavirus conocido como SARS-CoV-2. La OMS tuvo noticia por primera vez de la existencia de este nuevo virus el 31 de diciembre de 2019, al ser informada de un grupo de casos de «neumonía vírica» que se habían declarado en Wuhan (República Popular China).\n",
            "R1: ¿Cómo se propaga la COVID-19 entre las personas? La COVID-19 es una enfermedad causada por el virus SARS-CoV-2, que se propaga entre las personas principalmente cuando una persona infectada está en contacto cercano con otra persona. El virus se puede propagar a través de pequeñas partículas líquidas expulsadas por una persona infectada a través de la boca o la nariz al toser, estornudar, hablar, cantar o resoplar. Esas partículas líquidas tienen diferentes tamaños, desde las más grandes ‘gotículas respiratorias’ hasta las más pequeñas, llamadas ‘aerosoles’.  Otras personas pueden contraer la COVID-19 cuando el virus entra por la boca, la nariz o los ojos, algo que puede ocurrir con mayor probabilidad si las personas están en contacto directo o cercano (menos de 1 metro de distancia) con una persona infectada. Los datos actuales sugieren que el virus se propaga principalmente por medio de gotículas respiratorias entre personas que estén en contacto cercano. La transmisión por aerosoles puede producirse en entornos específicos, sobre todo en espacios interiores, abarrotados y mal ventilados en los que personas infectadas pasan mucho tiempo con otras, por ejemplo restaurantes, prácticas de coro, clases de gimnasia, clubes nocturnos, oficinas y/o lugares de culto. Se están realizando más estudios para comprender mejor las condiciones en las que se produce la transmisión por aerosoles fuera de los centros médicos en los que se realizan procedimientos médicos específicos llamados procedimientos generadores de aerosoles. El virus también se puede propagar cuando personas infectadas estornudan o tosen sobre superficies u objetos tales como mesas, picaportes o pasamanos, o tocan esas superficies. Otras personas se pueden infectar al tocar esas superficies contaminadas y luego tocarse los ojos, la nariz o la boca sin antes haberse lavado las manos. Para más información científica sobre la manera en que el virus SARS-CoV-2 infecta el organismo, así como sobre la reacción del sistema inmunitario de nuestro cuerpo, mire o lea esta  entrevista  con la Dra. Maria Van Kerkhove, directora técnica de la OMS para la COVID-19.\n",
            "\n",
            "A: cuando una persona infectada está en contacto cercano con otra persona\n",
            "Score: 0.12510879337787628\n",
            "Context Doc used: R1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}