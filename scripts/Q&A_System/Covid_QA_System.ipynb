{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdNqAh6NEIB6"
   },
   "source": [
    "# COVID Q&A System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESqoeGKXENSK"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Load libraries and document corpus to use as context for the Q&A System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nqWoHMktEM0S"
   },
   "outputs": [],
   "source": [
    "# TO RUN ON GOOGLE COLAB\n",
    "\n",
    "# Install transformers\n",
    "#!pip install transformers\n",
    "# For french\n",
    "#!pip install --no-cache-dir transformers sentencepiece\n",
    "\n",
    "# Change dir\n",
    "#%cd drive/MyDrive/NLP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "609oXVgDEAYS",
    "outputId": "123062c2-89b4-4636-8ba5-620d9430af0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CESAR\\Anaconda3\\envs\\tf-env\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\CESAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CESAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for excecution\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# NLP\n",
    "from gensim import corpora, models, similarities\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils\n",
    "\n",
    "# Transformer Models\n",
    "from transformers import pipeline\n",
    "\n",
    "# Display\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 10 threads to load 220 documents from WHO_CDC in es\n",
      "Loaded 220 files in 0.33 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 1000 documents from news in es\n",
      "Loaded 1000 files in 30.74 seconds.\n",
      "Removed 0 files becasuse they were too large\n"
     ]
    }
   ],
   "source": [
    "# Path to directory of docs\n",
    "path_prefix = r'D:\\Cesard\\Documents\\NLP'\n",
    "lang = 'es'\n",
    "\n",
    "# Instanciate utils class\n",
    "utils = Utils(path_prefix, num_workers=10)\n",
    "\n",
    "# Load WHO and CDC Docs\n",
    "WHO_CDC_text, _, WHO_CDC_titles = utils.data_loader(lang, 'WHO_CDC', total_data=None, max_size = None, return_titles = True )\n",
    "\n",
    "# Load News\n",
    "news_text, _, news_titles = utils.data_loader(lang, 'news', total_data=1000, max_size=None, return_titles = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append lists\n",
    "doc_text = WHO_CDC_text #+ news_text\n",
    "doc_titles = WHO_CDC_titles #+ news_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mcn-YMjrTDi"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Standard preprocesseing to all documents and queries for the IR task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7xs212v4rSGQ"
   },
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "# Stemmers\n",
    "stem = SnowballStemmer('spanish')\n",
    "#p_stem = PorterStemmer()\n",
    "\n",
    "# Tokenizers\n",
    "#tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "tk = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Lemmatizer\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Create vocab (dictionary)\n",
    "doc_dict = []\n",
    "for doc in doc_text:\n",
    "    doc_dict.append(utils.preprocessing(text=doc, stop_words = stop_words,\n",
    "                                                  stemmer = None,\n",
    "                                                  tokenizer = tk,\n",
    "                                                  lemmatizer = lemma))\n",
    "# Get dict\n",
    "dictionary = corpora.Dictionary(doc_dict)\n",
    "\n",
    "# Create doc corpus\n",
    "doc_corpus = []\n",
    "for doc in doc_text:\n",
    "    doc_corpus.append(dictionary.doc2bow(utils.preprocessing(text=doc, stop_words = stop_words,\n",
    "                                                                       stemmer = None,\n",
    "                                                                       tokenizer = tk,\n",
    "                                                                       lemmatizer = lemma)))\n",
    "    \n",
    "# Create title corpus\n",
    "title_corpus = []\n",
    "for title in doc_titles:\n",
    "    title_corpus.append(dictionary.doc2bow(utils.preprocessing(text=title, stop_words = stop_words,\n",
    "                                                                           stemmer = None,\n",
    "                                                                           tokenizer = tk,\n",
    "                                                                           lemmatizer = lemma)))\n",
    "\n",
    "# Serializes and saves dictionary and corpus files\n",
    "#dictionary.save('vocab.dict')\n",
    "#corpora.MmCorpus.serialize(\"covid_qa_corpus.mm\", doc_corpus)\n",
    "#corpora.MmCorpus.serialize(\"covid_qa_title.mm\", title_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TKWH-As2shGF"
   },
   "outputs": [],
   "source": [
    "# Load vocabulary, doc_corpus, query_corpus and df with tags\n",
    "#vocabulary = corpora.Dictionary.load('vocab.dict')\n",
    "#doc_corpus = corpora.MmCorpus(\"covid_qa_corpus.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHXHwu_3y6z9"
   },
   "source": [
    "## Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUobIgF_2Y81"
   },
   "source": [
    "### Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SI-jePThcUka",
    "outputId": "7e5057da-6ed7-4e25-a548-a73d893f38bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Doc Example in tfidf form: \n",
      "[(0, 0.4533891098472363), (1, 0.022260663916602915), (2, 0.03410059404562253), (3, 0.2473078033004329), (4, 0.14636309914918055), (5, 0.17699739729504257), (6, 0.026523344241058795), (7, 0.47319269372308664), (8, 0.17691435678393277), (9, 0.30692274190265945), (10, 0.08645164073613239), (11, 0.22322660661206623), (12, 0.12112776838018058), (13, 0.04996133546219115), (14, 0.10374196888335883), (15, 0.13808681039256607), (16, 0.049746801728317705), (17, 0.08612599664338606), (18, 0.15885807781038297), (19, 0.19323745128960043), (20, 0.18879794974705807), (21, 0.20426392385489597), (22, 0.11262310547759184), (23, 0.08448621272719399), (24, 0.23407626506077184)]\n"
     ]
    }
   ],
   "source": [
    "# Create tfidf model for document corpus\n",
    "tfidf = models.TfidfModel(doc_corpus)\n",
    "tfidf_title = models.TfidfModel(title_corpus)\n",
    "\n",
    "# Model transformation\n",
    "print('Title Doc Example in tfidf form: ')\n",
    "print(tfidf_title[doc_corpus][0]) \n",
    "\n",
    "# Similarity Matrix\n",
    "index = similarities.MatrixSimilarity(tfidf[doc_corpus])\n",
    "index_title = similarities.MatrixSimilarity(tfidf_title[title_corpus])\n",
    "\n",
    "# Save index\n",
    "#index.save('similarity_matrix.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zhCHywEE2cAO"
   },
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "A5Mn0w3M2Me_"
   },
   "outputs": [],
   "source": [
    "def perform_query(query, top_n = -1):\n",
    "    \"\"\" Perform IR over the corpus with the provided query using gensim tfidf model.\n",
    "    Args:\n",
    "        query (str): raw query (from user input)\n",
    "        top_n (int): max number of docs to retrieve\n",
    "    Returns:\n",
    "        context_doc_ids (list): List with the ids of the context docs\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess query\n",
    "    processed_query = utils.preprocessing(query, \n",
    "                                          stop_words = stop_words,\n",
    "                                          stemmer = None,\n",
    "                                          tokenizer = tk,\n",
    "                                          lemmatizer = lemma)\n",
    "\n",
    "    # Similarity between all docs and query\n",
    "    #sims = list(enumerate(index[tfidf[dictionary.doc2bow(processed_query)]]))\n",
    "\n",
    "    # Similarity between all doc titles and query\n",
    "    sims = list(enumerate(index_title[tfidf_title[dictionary.doc2bow(processed_query)]]))\n",
    "    \n",
    "    dtype = [('doc_id', int), ('score', float)]\n",
    "    doc_sims = np.array(sims, dtype=dtype)\n",
    "    \n",
    "    # Sort Docs by similarity\n",
    "    doc_sims_sorted = np.flip(np.sort(doc_sims, order='score'))\n",
    "\n",
    "    # Retrieve only documents with non zero score\n",
    "    k = len(np.nonzero(doc_sims['score'])[0])\n",
    "    relevant_docs = doc_sims_sorted[0:k]\n",
    "    \n",
    "    # Print only top docs\n",
    "    context_doc_ids = relevant_docs['doc_id'][0:top_n]\n",
    "    \n",
    "    return context_doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFcvo0mJF7wd"
   },
   "source": [
    "## Deep Model\n",
    "\n",
    "Load deep pre-trained model for Q&A to extract the answer from the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SSvbU4gCGLBj"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained Q&A Model for lang\n",
    "if lang == 'en':\n",
    "    covid_qa = pipeline(\"question-answering\", model='deepset/roberta-base-squad2-covid')\n",
    "elif lang == 'es':\n",
    "    covid_qa = pipeline(\"question-answering\", model='mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es')\n",
    "elif lang == 'fr':\n",
    "    covid_qa = pipeline('question-answering', model='fmikaelian/camembert-base-fquad', tokenizer='fmikaelian/camembert-base-fquad', use_fast = False)\n",
    "else:\n",
    "    print('Not supported language')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "faIZjbVFF7_N"
   },
   "outputs": [],
   "source": [
    "def perform_qa(query, context_ids, confidence_th = 0.05):\n",
    "    \"\"\" Run QA Deep model with query and context from param.\n",
    "    Args:\n",
    "        query (str): raw query (from user input)\n",
    "        context_id (int): document id with the context.\n",
    "    Returns:\n",
    "        result (dir): Answer provided by the model with confidence score.\n",
    "        k (int): Number of context documents found\n",
    "    \"\"\"\n",
    "\n",
    "    # Get context from retrieved docs\n",
    "    for k, id in enumerate(context_ids):\n",
    "        # Get context doc\n",
    "        context = doc_titles[id] + ' ' + doc_text[id]\n",
    "\n",
    "        # Perform QA\n",
    "        result = covid_qa(question=query, context=context)\n",
    "\n",
    "        # If score is good enough return result\n",
    "        if result['score'] > confidence_th:\n",
    "            return result, k\n",
    "    \n",
    "    # Default response if not good enough answer nor context was found\n",
    "    if lang == 'en':\n",
    "        if not len(context_ids):\n",
    "            result = {'answer': 'The context of that question is outside of my domain', 'score': 0}\n",
    "        else:\n",
    "            result = {'answer': 'Sorry, the answer to that question was not found', 'score': 0}\n",
    "    elif lang == 'es':\n",
    "        if not len(context_ids):\n",
    "            result = {'answer': 'El contexto de esa pregunta esta fuera de mi dominio', 'score': 0}\n",
    "        else:\n",
    "            result = {'answer': 'Disculpe, la respuesta a esa pregunta no se encontró', 'score': 0}\n",
    "    elif lang == 'fr':\n",
    "        if not len(context_ids):\n",
    "            result = {'answer': 'Le contexte de cette question est en dehors de mon domaine', 'score': 0}\n",
    "        else:\n",
    "            result = {'answer': 'Pardon, la réponse à cette question n\\'a pas été trouvée', 'score': 0}\n",
    "    else:\n",
    "        print('Not supported language')\n",
    "\n",
    "    return result, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhQyxcOk3vL0"
   },
   "source": [
    "## Q&A Demo\n",
    "\n",
    "Demo of the two stage system (IR and Q&A) with some example questions or input from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kC-7xCO2tW3e"
   },
   "outputs": [],
   "source": [
    "# Example questions \n",
    "q_en = [\"What is COVID-19?\",\n",
    "        \"How does COVID-19 spreads?\",\n",
    "        \"What symptoms does covid-19 causes?\", \n",
    "        \"Which kind of mask should I use to protect me from covid?\",\n",
    "        \"How should I protect from covid?\",\n",
    "        \"Should I get a vaccine for covid-19?\"]\n",
    "\n",
    "q_es = [\"¿Qué es el COVID-19?\",\n",
    "        \"¿Cómo se propaga el virus?\",\n",
    "        \"¿Cuales son los síntomas del covid-19?\", \n",
    "        \"¿Qué tipo de máscara debería utilizar para protegerme del covid?\",\n",
    "        \"¿Cómo protegerse del covid?\",\n",
    "        \"¿Debería vacunarme si ya tuve covid-19?\",\n",
    "        \"¿La vacuna de Astrazeneca causa trombos?\",\n",
    "        \"Qué distancia debo mantener para protegerme del covid?\"]\n",
    "\n",
    "q_fr = [\"Qu'est-ce que COVID-19?\",\n",
    "        \"Comment le COVID-19 se propage-t-il?\",\n",
    "        \"Quels symptômes le covid-19 provoque-t-il?\",\n",
    "        \"Quel type de masque dois-je utiliser pour me protéger du covid?\",\n",
    "        \"Comment dois-je me protéger de Covid?\",\n",
    "        \"Dois-je me faire vacciner contre le covid-19?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmAL9QyX32bm",
    "outputId": "d03a5bae-c413-4e7d-dff9-a8f8c00821da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: ¿Qué es el COVID-19?\n",
      "\n",
      "Context Docs (Ranked): \n",
      "R0: ¿Qué es el COVID-19​​​​​​​?\n",
      "El COVID-19 es una enfermedad causada por un virus llamado SARS-CoV-2. La mayoría de las personas con COVID-19 tienen  síntomas  leves, pero algunas personas pueden enfermarse gravemente. Aunque la mayoría de las personas con COVID-19 mejora al cabo de unas semanas de haber estado enfermas, algunas personas experimentan afecciones posteriores al COVID-19. Las  afecciones posteriores al COVID-19  son una amplia variedad de problemas de salud nuevos, recurrentes o en curso que las personas pueden experimentar  más de cuatro semanas  después de haberse infectado por primera vez por el virus que causa el COVID-19. Las personas mayores y las personas que tienen  ciertas afecciones subyacentes  tienen mayor riesgo de enfermarse gravemente a causa del COVID-19.  Las vacunas  contra el COVID-19 son seguras y efectivas.\n",
      "\n",
      "A: una enfermedad causada por un virus llamado SARS-CoV-2\n",
      "Score: 0.6277936697006226\n",
      "Context Doc used: R0\n"
     ]
    }
   ],
   "source": [
    "# Select one random question\n",
    "q = q_es[0]\n",
    "\n",
    "# Use input from user\n",
    "#q = input('COVID Q&A: ')\n",
    "\n",
    "print(f\"Q: {q}\")\n",
    "print('')\n",
    "\n",
    "# Information Retrieval (get context)\n",
    "context_ids = perform_query(q)\n",
    "\n",
    "# Question Answering\n",
    "result, k = perform_qa(q, context_ids, confidence_th = 0.1)\n",
    "\n",
    "# Print results\n",
    "print(f\"Context Docs (Ranked): \")\n",
    "for i, id in enumerate(context_ids):\n",
    "    print(f\"R{i}: {doc_titles[id]}\")\n",
    "    if i >= k:\n",
    "        print(doc_text[id])\n",
    "        break\n",
    "print('')\n",
    "\n",
    "print(f\"A: {result['answer']}\")\n",
    "print(f\"Score: {result['score']}\")\n",
    "print(f\"Context Doc used: R{k}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Covid_QA_System.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
