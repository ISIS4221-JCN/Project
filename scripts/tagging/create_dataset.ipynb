{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669843ce",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4179d224",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef39485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import torch, re\n",
    "\n",
    "# Display\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b730bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "lang = 'en'\n",
    "emb = 'doc2vec'\n",
    "tag_type = 'keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8ff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils\n",
    "\n",
    "# Instanciate utils class\n",
    "utils = Utils('/media/juan/Juan/NLP/', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7761528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load manual tagged data...\n",
      "Starting 10 threads to load 201 documents from reddit in en\n",
      "Loaded 201 files in 0.02 seconds.\n",
      "Starting 10 threads to load 454 documents from tweets in en\n",
      "Loaded 454 files in 0.05 seconds.\n",
      "Starting 10 threads to load 100 documents from news in en\n",
      "Loaded 100 files in 0.01 seconds.\n",
      "Loaded 454 tagged Tweets 201 and tagged Reddit docs\n",
      "\n",
      "Starting to load keyword tagged data...\n",
      "Starting 10 threads to load 30233 documents from reddit in en\n",
      "Loaded 30233 files in 3.27 seconds.\n",
      "Starting 10 threads to load 114795 documents from tweets in en\n",
      "Loaded 114795 files in 17.68 seconds.\n",
      "Loaded 114795 tagged Tweets 30233 and tagged Reddit docs\n",
      "Starting to load not tagged data...\n",
      "Starting 10 threads to load 10000 documents from reddit in en\n",
      "Loaded 10000 files in 43.35 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 10000 documents from tweets in en\n",
      "Loaded 10000 files in 32.03 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 10000 documents from news in en\n",
      "Loaded 10000 files in 283.12 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Loaded 10000 Tweets 10000, Reddit docs and 10000 docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load manual tags\n",
    "with open(f'tags/manual/reddit_{lang}.json', 'r+') as file_str:\n",
    "    reddit_manual_tags = json.load(file_str)\n",
    "with open(f'tags/manual/tweets_{lang}.json', 'r+') as file_str:\n",
    "    twitter_manual_tags = json.load(file_str)\n",
    "with open(f'tags/manual/news_{lang}.json', 'r+') as file_str:\n",
    "    news_manual_tags = json.load(file_str)\n",
    "\n",
    "# Load keyword tags\n",
    "with open(f'tags/{tag_type}/reddit_{lang}_words.json', 'r+') as file_str:\n",
    "    reddit_enhanced_tags = json.load(file_str)\n",
    "with open(f'tags/{tag_type}/tweets_{lang}_words.json', 'r+') as file_str:\n",
    "    twitter_enhanced_tags = json.load(file_str)\n",
    "\n",
    "# Load Tagged data\n",
    "print('Starting to load manual tagged data...')\n",
    "manual_reddit_data, manual_reddit_file_names = utils.tagged_data_loader(list(reddit_manual_tags.keys()), 'reddit', lang)\n",
    "manual_twitter_data, manual_twitter_file_names = utils.tagged_data_loader(list(twitter_manual_tags.keys()), 'tweets', lang)\n",
    "manual_news_data, manual_news_file_names = utils.tagged_data_loader(list(news_manual_tags.keys()), 'news', lang)\n",
    "print(f'Loaded {len(manual_twitter_data)} tagged Tweets {len(manual_reddit_data)} and tagged Reddit docs')\n",
    "print('')\n",
    "\n",
    "# Load Enhanced Tagged data\n",
    "print('Starting to load keyword tagged data...')\n",
    "enhanced_reddit_data, enhanced_reddit_file_names = utils.tagged_data_loader(list(reddit_enhanced_tags.keys()), 'reddit', lang)\n",
    "enhanced_twitter_data, enhanced_twitter_file_names = utils.tagged_data_loader(list(twitter_enhanced_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(enhanced_twitter_data)} tagged Tweets {len(enhanced_reddit_data)} and tagged Reddit docs')\n",
    "\n",
    "# Load Not Tagged data\n",
    "print('Starting to load not tagged data...')\n",
    "reddit_data, _, _ = utils.data_loader(lang, 'reddit', total_data=10000, max_size = None)\n",
    "twitter_data, _, _ = utils.data_loader(lang, 'tweets', total_data=10000, max_size = None)\n",
    "news_data, _, _= utils.data_loader(lang, 'news', total_data=10000, max_size = None)\n",
    "print(f'Loaded {len(reddit_data)} Tweets {len(twitter_data)}, Reddit docs and {len(news_data)} docs')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "663cfac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lists\n",
    "manual_tags = {**twitter_manual_tags , **reddit_manual_tags} #,**news_manual_tags}\n",
    "manual_tagged_data = manual_twitter_data + manual_reddit_data #+ manual_news_data\n",
    "manual_tagged_file_names = manual_twitter_file_names + manual_reddit_file_names #+ manual_news_\n",
    "\n",
    "enhanced_tags = {**twitter_enhanced_tags , **reddit_enhanced_tags}\n",
    "enhanced_tagged_data = enhanced_twitter_data + enhanced_reddit_data\n",
    "enhanced_tagged_file_names = enhanced_twitter_file_names + enhanced_reddit_file_names\n",
    "\n",
    "extended_data = set(twitter_data + reddit_data + news_data) - set(manual_tagged_data + enhanced_tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb1f2f9",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83be2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_social(data, language='en'):\n",
    "    \n",
    "    # Creates the language dictionary\n",
    "    lang_dict = {\n",
    "        \"en\": \"english\",\n",
    "        \"es\": \"spanish\",\n",
    "        \"fr\": \"french\"\n",
    "    }\n",
    "    \n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    # Sets text into lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Tokenizes by word\n",
    "    tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    data = tk.tokenize(data)\n",
    "    \n",
    "    data_temp = []\n",
    "    for word in data:\n",
    "        if word not in string.punctuation:\n",
    "            data_temp.append(word)\n",
    "    data = data_temp\n",
    "    \n",
    "    # Removes stopwords\n",
    "    data = [token for token in data if token not in stopwords.words(lang_dict[language])]\n",
    "    \n",
    "    # Creates the stemmer\n",
    "    stemmer = SnowballStemmer(lang_dict[language])\n",
    "    \n",
    "    # Stems data\n",
    "    data = [stemmer.stem(token) for token in data]\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b0e43",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1659c42d",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0882ac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Models\n",
    "#model_name = \"microsoft/xtremedistil-l6-h384-uncased\"\n",
    "model_name = \"Darkrider/covidbert_medmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=False)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2a2feed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(data):\n",
    "    \n",
    "    # Preprocess data\n",
    "    corpus = []\n",
    "    print('Preprocessing data...')\n",
    "    for d in tqdm(data):\n",
    "        corpus.append(preprocess_social(d, language=lang))\n",
    "    clear_output()\n",
    "    \n",
    "    # Array to save embeddings\n",
    "    embeddings = []\n",
    "    failed_doc_id = []\n",
    "    print('Building embeddings...')\n",
    "    for i, doc in enumerate(tqdm(corpus)):\n",
    "        try:\n",
    "            # Run Bert for each document\n",
    "            inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            # CLS Token Output\n",
    "            embedding = outputs['pooler_output'].detach().cpu().numpy()[0]\n",
    "            # Append representation\n",
    "            embeddings.append(embedding)\n",
    "        except:\n",
    "            failed_doc_id.append(i)\n",
    "    clear_output()\n",
    "    print(f\"Created embeddings for {len(embeddings)} docs and fail to create {len(failed_doc_id)} embeddings\")\n",
    "            \n",
    "    return embeddings, failed_doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d8759",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb639b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "path_to_model = '/media/juan/Juan/NLP/models/doc2vec_' + lang + '.model'\n",
    "d2v_model = Doc2Vec.load(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5cea254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_embeddings(data):\n",
    "    \n",
    "    # Preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    corpus = []\n",
    "    for d in tqdm(data):\n",
    "        corpus.append(preprocess_social(d, language=lang))\n",
    "    clear_output()\n",
    "    \n",
    "    print('Building embeddings...')\n",
    "    embeddings = []\n",
    "    for doc in tqdm(corpus):    \n",
    "        embeddings.append(d2v_model.infer_vector(doc))\n",
    "    clear_output()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3588c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tags\n",
    "\n",
    "# Create manual tags matrix for testing\n",
    "y_test = np.zeros((len(manual_tags), 3))\n",
    "\n",
    "for i, file_name in enumerate(manual_tagged_file_names):\n",
    "    if list(manual_tags[file_name].values())[1] or list(manual_tags[file_name].values())[2]:\n",
    "        y_test[i][0] = 1\n",
    "    elif list(manual_tags[file_name].values())[3] or list(manual_tags[file_name].values())[4]:\n",
    "        y_test[i][1] = 1\n",
    "    else:\n",
    "        y_test[i][2] = 1\n",
    "            \n",
    "# Create enhanced tags matrix for training\n",
    "y_aux = np.zeros((len(enhanced_tagged_data) + len(extended_data), 3))\n",
    "\n",
    "for i, file_name in enumerate(enhanced_tagged_file_names):\n",
    "    if list(enhanced_tags[file_name].values())[1] or list(enhanced_tags[file_name].values())[2]:\n",
    "        y_aux[i][0] = 1\n",
    "    elif list(enhanced_tags[file_name].values())[3] or list(enhanced_tags[file_name].values())[4]:\n",
    "        y_aux[i][1] = 1\n",
    "    else:\n",
    "        y_aux[i][2] = 1\n",
    "\n",
    "# Add tag to last position on array if not tagged\n",
    "for i, y in enumerate(y_test):\n",
    "    if not sum(y):\n",
    "        y[-1] = 1\n",
    "\n",
    "for i, y in enumerate(y_aux):\n",
    "    if not sum(y):\n",
    "        y[-1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aad7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Embeddings \n",
    "if emb == 'bert':\n",
    "    # Test input embeddings\n",
    "    X_test, f_test_id = get_bert_embedding(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings, f_enhanced_id = get_bert_embedding(enhanced_tagged_data)\n",
    "    extended_embeddings, f_extended_id = get_bert_embedding(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings\n",
    "    \n",
    "    # Remove failed docs\n",
    "    failed_doc_id = f_enhanced_id + f_extended_id\n",
    "\n",
    "    y_test = np.delete(y_test, f_test_id, axis=0)\n",
    "    y_aux = np.delete(y_aux, failed_doc_id, axis=0)\n",
    "    \n",
    "elif emb == 'doc2vec':\n",
    "    # Test input embeddings\n",
    "    X_test = get_doc2vec_embeddings(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings = get_doc2vec_embeddings(enhanced_tagged_data)\n",
    "    extended_embeddings = get_doc2vec_embeddings(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428a900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bc5e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "# Input\n",
    "np.save(f'/media/juan/Juan/NLP/datasets/X_train_{emb}_{lang}.npy', X_train)\n",
    "np.save(f'/media/juan/Juan/NLP/datasets/X_val_{emb}_{lang}.npy', X_val)\n",
    "np.save(f'/media/juan/Juan/NLP/datasets/X_test_{emb}_{lang}.npy', X_test)\n",
    "# Tags\n",
    "np.save(f'/media/juan/Juan/NLP/datasets/y_train_{emb}_{lang}.npy', y_train)\n",
    "np.save(f'/media/juan/Juan/NLP/datasets/y_val_{emb}_{lang}.npy', y_val)\n",
    "np.save(f'/media/juan/Juan/NLP/datasets/y_test_{emb}_{lang}.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
