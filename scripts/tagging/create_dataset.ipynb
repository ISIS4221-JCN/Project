{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71468e5b",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8fa558",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18337cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import torch, re\n",
    "\n",
    "# Display\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2420c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "lang = 'es'\n",
    "emb = 'bert'\n",
    "tag_type = 'keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e77cb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils\n",
    "\n",
    "# Instanciate utils class\n",
    "utils = Utils(r'D:\\Cesard\\Documents\\NLP', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15b676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load manual tagged data...\n",
      "Starting 10 threads to load 1500 documents from reddit in es\n",
      "Loaded 1500 files in 0.17 seconds.\n",
      "Starting 10 threads to load 1000 documents from tweets in es\n",
      "Loaded 1000 files in 0.10 seconds.\n",
      "Loaded 1000 tagged Tweets 1500 and tagged Reddit docs\n",
      "\n",
      "Starting to load keyword tagged data...\n",
      "Starting 10 threads to load 2283 documents from reddit in es\n",
      "Loaded 2283 files in 0.26 seconds.\n",
      "Starting 10 threads to load 35239 documents from tweets in es\n",
      "Loaded 35239 files in 5.97 seconds.\n",
      "Loaded 35239 tagged Tweets 2283 and tagged Reddit docs\n",
      "Starting to load not tagged data...\n",
      "Starting 10 threads to load 9113 documents from reddit in es\n",
      "Loaded 9113 files in 0.98 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 10000 documents from tweets in es\n",
      "Loaded 10000 files in 56.72 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Loaded 9113 tagged Tweets 10000 and tagged Reddit docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load manual tags\n",
    "with open(f'tags/manual/reddit_{lang}.json', 'r+') as file_str:\n",
    "    reddit_manual_tags = json.load(file_str)\n",
    "with open(f'tags/manual/tweets_{lang}.json', 'r+') as file_str:\n",
    "    twitter_manual_tags = json.load(file_str)\n",
    "#with open(f'tags/manual/news_{lang}.json', 'r+') as file_str:\n",
    "#    news_manual_tags = json.load(file_str)\n",
    "\n",
    "# Load keyword tags\n",
    "with open(f'tags/{tag_type}/reddit_{lang}_words.json', 'r+') as file_str:\n",
    "    reddit_enhanced_tags = json.load(file_str)\n",
    "with open(f'tags/{tag_type}/tweets_{lang}_words.json', 'r+') as file_str:\n",
    "    twitter_enhanced_tags = json.load(file_str)\n",
    "\n",
    "# Load Tagged data\n",
    "print('Starting to load manual tagged data...')\n",
    "manual_reddit_data, manual_reddit_file_names = utils.tagged_data_loader(list(reddit_manual_tags.keys()), 'reddit', lang)\n",
    "manual_twitter_data, manual_twitter_file_names = utils.tagged_data_loader(list(twitter_manual_tags.keys()), 'tweets', lang)\n",
    "manual_news_data, manual_news_file_names = utils.tagged_data_loader(list(twitter_news_tags.keys()), 'news', lang)\n",
    "print(f'Loaded {len(manual_twitter_data)} tagged Tweets {len(manual_reddit_data)} and tagged Reddit docs')\n",
    "print('')\n",
    "\n",
    "# Load Enhanced Tagged data\n",
    "print('Starting to load keyword tagged data...')\n",
    "enhanced_reddit_data, enhanced_reddit_file_names = utils.tagged_data_loader(list(reddit_enhanced_tags.keys()), 'reddit', lang)\n",
    "enhanced_twitter_data, enhanced_twitter_file_names = utils.tagged_data_loader(list(twitter_enhanced_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(enhanced_twitter_data)} tagged Tweets {len(enhanced_reddit_data)} and tagged Reddit docs')\n",
    "\n",
    "# Load Not Tagged data\n",
    "print('Starting to load not tagged data...')\n",
    "reddit_data, _, _ = utils.data_loader(lang, 'reddit', total_data=None, max_size = None)\n",
    "twitter_data, _, _ = utils.data_loader(lang, 'tweets', total_data=10000, max_size = None)\n",
    "news_data, _, = utils.data_loader(lang, 'news', total_data=10000, max_size = None)\n",
    "print(f'Loaded {len(reddit_data)} Tweets {len(twitter_data)}, Reddit docs and {len(news_data)} docs')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c728f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lists\n",
    "manual_tags = {**twitter_manual_tags , **reddit_manual_tags} #,**news_manual_tags}\n",
    "manual_tagged_data = manual_twitter_data + manual_reddit_data #+ manual_news_data\n",
    "manual_tagged_file_names = manual_twitter_file_names + manual_reddit_file_names #+ manual_news_\n",
    "\n",
    "enhanced_tags = {**twitter_enhanced_tags , **reddit_enhanced_tags}\n",
    "enhanced_tagged_data = enhanced_twitter_data + enhanced_reddit_data\n",
    "enhanced_tagged_file_names = enhanced_twitter_file_names + enhanced_reddit_file_names\n",
    "\n",
    "extended_data = set(twitter_data + reddit_data + news_data) - set(manual_tagged_data + enhanced_tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300e0de",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cf5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_social(data, language='en'):\n",
    "    \n",
    "    # Creates the language dictionary\n",
    "    lang_dict = {\n",
    "        \"en\": \"english\",\n",
    "        \"es\": \"spanish\",\n",
    "        \"fr\": \"french\"\n",
    "    }\n",
    "    \n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    # Sets text into lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Tokenizes by word\n",
    "    tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    data = tk.tokenize(data)\n",
    "    \n",
    "    data_temp = []\n",
    "    for word in data:\n",
    "        if word not in string.punctuation:\n",
    "            data_temp.append(word)\n",
    "    data = data_temp\n",
    "    \n",
    "    # Removes stopwords\n",
    "    data = [token for token in data if token not in stopwords.words(lang_dict[language])]\n",
    "    \n",
    "    # Creates the stemmer\n",
    "    stemmer = SnowballStemmer(lang_dict[language])\n",
    "    \n",
    "    # Stems data\n",
    "    data = [stemmer.stem(token) for token in data]\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327dda1",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc4073",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9f9e02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Models\n",
    "#model_name = \"microsoft/xtremedistil-l6-h384-uncased\"\n",
    "model_name = \"Darkrider/covidbert_medmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=False)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469d5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(data):\n",
    "    \n",
    "    # Preprocess data\n",
    "    corpus = []\n",
    "    print('Preprocessing data...')\n",
    "    for d in tqdm(data):\n",
    "        corpus.append(preprocess_social(d, language=lang))\n",
    "    clear_output()\n",
    "    \n",
    "    # Array to save embeddings\n",
    "    embeddings = []\n",
    "    failed_doc_id = []\n",
    "    print('Building embeddings...')\n",
    "    for i, doc in enumerate(tqdm(corpus)):\n",
    "        try:\n",
    "            # Run Bert for each document\n",
    "            inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            # CLS Token Output\n",
    "            embedding = outputs['pooler_output'].detach().cpu().numpy()[0]\n",
    "            # Append representation\n",
    "            embeddings.append(embedding)\n",
    "        except:\n",
    "            failed_doc_id.append(i)\n",
    "    clear_output()\n",
    "    print(f\"Created embeddings for {len(embeddings)} docs and fail to create {len(failed_doc_id)} embeddings\")\n",
    "            \n",
    "    return embeddings, failed_doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47163a",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8b813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "path_to_model = r'D:\\Cesard\\Documents\\NLP\\models\\doc2vec_es.model'\n",
    "d2v_model = Doc2Vec.load(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a5ce66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_embeddings(data):\n",
    "    \n",
    "    # Preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    corpus = []\n",
    "    for d in tqdm(data):\n",
    "        corpus.append(preprocess_social(d, language=lang))\n",
    "    clear_output()\n",
    "    \n",
    "    print('Building embeddings...')\n",
    "    embeddings = []\n",
    "    for doc in tqdm(corpus):    \n",
    "        embeddings.append(d2v_model.infer_vector(doc))\n",
    "    clear_output()\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c848bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 14225 docs and fail to create 225 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Create manual tags matrix for testing\n",
    "y_test = np.zeros((len(manual_tags), 5))\n",
    "\n",
    "for i, file_name in enumerate(manual_tagged_file_names):\n",
    "    for j, tag in enumerate(list(manual_tags[file_name].values())):\n",
    "        if tag:\n",
    "            y_test[i][j] = 1\n",
    "            \n",
    "# Create enhanced tags matrix for training\n",
    "y_aux = np.zeros((len(enhanced_tagged_data) + len(extended_data), 5))\n",
    "\n",
    "for i, file_name in enumerate(enhanced_tagged_file_names):\n",
    "    for j, tag in enumerate(list(enhanced_tags[file_name].values())):\n",
    "        if tag:\n",
    "            y_aux[i][j] = 1\n",
    "\n",
    "# Add tag to last position on array if not tagged\n",
    "for i, y in enumerate(y_test):\n",
    "    if not sum(y):\n",
    "        y[-1] = 1\n",
    "\n",
    "for i, y in enumerate(y_aux):\n",
    "    if not sum(y):\n",
    "        y[-1] = 1\n",
    "\n",
    "# Get Embeddings \n",
    "if emb == 'bert':\n",
    "    # Test input embeddings\n",
    "    X_test, f_test_id = get_bert_embedding(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings, f_enhanced_id = get_bert_embedding(enhanced_tagged_data)\n",
    "    extended_embeddings, f_extended_id = get_bert_embedding(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings\n",
    "    \n",
    "    # Remove failed docs\n",
    "    failed_doc_id = f_enhanced_id + f_extended_id\n",
    "\n",
    "    y_test = np.delete(y_test, f_test_id, axis=0)\n",
    "    y_aux = np.delete(y_aux, failed_doc_id, axis=0)\n",
    "    \n",
    "elif emb == 'doc2vec':\n",
    "    # Test input embeddings\n",
    "    X_test = get_doc2vec_embeddings(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings = get_doc2vec_embeddings(enhanced_tagged_data)\n",
    "    extended_embeddings = get_doc2vec_embeddings(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9980e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06f12555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "# Input\n",
    "np.save(f'X_train_{emb}_{lang}.npy', X_train)\n",
    "np.save(f'X_val_{emb}_{lang}.npy', X_val)\n",
    "np.save(f'X_test_{emb}_{lang}.npy', X_test)\n",
    "# Tags\n",
    "np.save(f'y_train_{emb}_{lang}.npy', y_train)\n",
    "np.save(f'y_val_{emb}_{lang}.npy', y_val)\n",
    "np.save(f'y_test_{emb}_{lang}.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3665710c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 1, ..., 4, 4, 0], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
