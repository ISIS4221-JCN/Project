{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd7b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "LANG = 'en'\n",
    "EMB = 'bert'\n",
    "TAG_TYPE = 'doc2vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399cfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils\n",
    "\n",
    "# Instanciate utils class\n",
    "utils = Utils(r'D:\\Cesard\\Documents\\NLP', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb37d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manual tags\n",
    "with open(f'tags/manual/reddit_{lang}.json', 'r+') as file_str:\n",
    "    reddit_manual_tags = json.load(file_str)\n",
    "with open(f'tags/manual/tweets_{lang}.json', 'r+') as file_str:\n",
    "    twitter_manual_tags = json.load(file_str)\n",
    "\n",
    "# Load keyword tags\n",
    "with open(f'tags/keywords/reddit_{lang}_words.json', 'r+') as file_str:\n",
    "    reddit_kw_tags = json.load(file_str)\n",
    "with open(f'tags/keywords/tweets_{lang}_words.json', 'r+') as file_str:\n",
    "    twitter_kw_tags = json.load(file_str)\n",
    "\n",
    "# Load Tagged data\n",
    "print('Starting to load manual tagged data...')\n",
    "manual_reddit_data, manual_reddit_file_names = utils.tagged_data_loader(list(reddit_manual_tags.keys()), 'reddit', lang)\n",
    "manual_twitter_data, manual_twitter_file_names = utils.tagged_data_loader(list(twitter_manual_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(tagged_twitter_data)} tagged Tweets {len(tagged_reddit_data)} and tagged Reddit docs')\n",
    "print('')\n",
    "\n",
    "# Load Keyword tagged data\n",
    "print('Starting to load keyword tagged data...')\n",
    "reddit_data, reddit_file_names = utils.tagged_data_loader(list(reddit_kw_tags.keys()), 'reddit', lang)\n",
    "twitter_data, twitter_file_names = utils.tagged_data_loader(list(twitter_kw_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(twitter_data)} tagged Tweets {len(reddit_data)} and tagged Reddit docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9043e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
