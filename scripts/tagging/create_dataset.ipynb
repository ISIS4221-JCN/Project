{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842e5ac1",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df774e0",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5173566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import torch, re\n",
    "\n",
    "# Display\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b32c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "lang = 'es'\n",
    "emb = 'bert'\n",
    "tag_type = 'keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54944129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils\n",
    "\n",
    "# Instanciate utils class\n",
    "utils = Utils(r'D:\\Cesard\\Documents\\NLP', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e027f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load manual tagged data...\n",
      "Starting 10 threads to load 1500 documents from reddit in es\n",
      "Loaded 1500 files in 0.16 seconds.\n",
      "Starting 10 threads to load 1000 documents from tweets in es\n",
      "Loaded 1000 files in 0.10 seconds.\n",
      "Loaded 1000 tagged Tweets 1500 and tagged Reddit docs\n",
      "\n",
      "Starting to load keyword tagged data...\n",
      "Starting 10 threads to load 2283 documents from reddit in es\n",
      "Loaded 2283 files in 0.25 seconds.\n",
      "Starting 10 threads to load 35239 documents from tweets in es\n",
      "Loaded 35239 files in 3.64 seconds.\n",
      "Loaded 35239 tagged Tweets 2283 and tagged Reddit docs\n",
      "Starting to load not tagged data...\n",
      "Starting 10 threads to load 9113 documents from reddit in es\n",
      "Loaded 9113 files in 8.43 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 10000 documents from tweets in es\n",
      "Loaded 10000 files in 126.52 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Loaded 9113 tagged Tweets 10000 and tagged Reddit docs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load manual tags\n",
    "with open(f'tags/manual/reddit_{lang}.json', 'r+') as file_str:\n",
    "    reddit_manual_tags = json.load(file_str)\n",
    "with open(f'tags/manual/tweets_{lang}.json', 'r+') as file_str:\n",
    "    twitter_manual_tags = json.load(file_str)\n",
    "\n",
    "# Load keyword tags\n",
    "with open(f'tags/{tag_type}/reddit_{lang}_words.json', 'r+') as file_str:\n",
    "    reddit_enhanced_tags = json.load(file_str)\n",
    "with open(f'tags/{tag_type}/tweets_{lang}_words.json', 'r+') as file_str:\n",
    "    twitter_enhanced_tags = json.load(file_str)\n",
    "\n",
    "# Load Tagged data\n",
    "print('Starting to load manual tagged data...')\n",
    "manual_reddit_data, manual_reddit_file_names = utils.tagged_data_loader(list(reddit_manual_tags.keys()), 'reddit', lang)\n",
    "manual_twitter_data, manual_twitter_file_names = utils.tagged_data_loader(list(twitter_manual_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(manual_twitter_data)} tagged Tweets {len(manual_reddit_data)} and tagged Reddit docs')\n",
    "print('')\n",
    "\n",
    "# Load Enhanced Tagged data\n",
    "print('Starting to load keyword tagged data...')\n",
    "enhanced_reddit_data, enhanced_reddit_file_names = utils.tagged_data_loader(list(reddit_enhanced_tags.keys()), 'reddit', lang)\n",
    "enhanced_twitter_data, enhanced_twitter_file_names = utils.tagged_data_loader(list(twitter_enhanced_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(enhanced_twitter_data)} tagged Tweets {len(enhanced_reddit_data)} and tagged Reddit docs')\n",
    "\n",
    "# Load Not Tagged data\n",
    "print('Starting to load not tagged data...')\n",
    "reddit_data, _, _ = utils.data_loader(lang, 'reddit', total_data=None, max_size = None)\n",
    "twitter_data, _, _ = utils.data_loader(lang, 'tweets', total_data=10000, max_size = None)\n",
    "print(f'Loaded {len(reddit_data)} tagged Tweets {len(twitter_data)} and tagged Reddit docs')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b331731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lists\n",
    "manual_tags = {**twitter_manual_tags , **reddit_manual_tags}\n",
    "manual_tagged_data = manual_twitter_data + manual_reddit_data \n",
    "manual_tagged_file_names = manual_twitter_file_names + manual_reddit_file_names \n",
    "\n",
    "enhanced_tags = {**twitter_enhanced_tags , **reddit_enhanced_tags}\n",
    "enhanced_tagged_data = enhanced_twitter_data + enhanced_reddit_data\n",
    "enhanced_tagged_file_names = enhanced_twitter_file_names + enhanced_reddit_file_names\n",
    "\n",
    "extended_data = set(twitter_data + reddit_data) - set(manual_tagged_data + enhanced_tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a35d1b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b9a69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_social(data, language='en'):\n",
    "    \n",
    "    # Creates the language dictionary\n",
    "    lang_dict = {\n",
    "        \"en\": \"english\",\n",
    "        \"es\": \"spanish\",\n",
    "        \"fr\": \"french\"\n",
    "    }\n",
    "    \n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    # Sets text into lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Tokenizes by word\n",
    "    tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    data = tk.tokenize(data)\n",
    "    \n",
    "    data_temp = []\n",
    "    for word in data:\n",
    "        if word not in string.punctuation:\n",
    "            data_temp.append(word)\n",
    "    data = data_temp\n",
    "    \n",
    "    # Removes stopwords\n",
    "    data = [token for token in data if token not in stopwords.words(lang_dict[language])]\n",
    "    \n",
    "    # Creates the stemmer\n",
    "    stemmer = SnowballStemmer(lang_dict[language])\n",
    "    \n",
    "    # Stems data\n",
    "    data = [stemmer.stem(token) for token in data]\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a63119",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0db87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Models\n",
    "#model_name = \"microsoft/xtremedistil-l6-h384-uncased\"\n",
    "model_name = \"Darkrider/covidbert_medmarco\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=False)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79e4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(data):\n",
    "    \n",
    "    # Preprocess data\n",
    "    corpus = []\n",
    "    for d in data:\n",
    "        corpus.append(preprocess_social(d, language=lang))\n",
    "        \n",
    "    # Array to save embeddings\n",
    "    embeddings = []\n",
    "    failed_doc_id = []\n",
    "    for i, doc in enumerate(corpus):\n",
    "        try:\n",
    "            # Run Bert for each document\n",
    "            inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            # CLS Token Output\n",
    "            embedding = outputs['pooler_output'].detach().cpu().numpy()[0]\n",
    "            # Append representation\n",
    "            embeddings.append(embedding)\n",
    "        except:\n",
    "            failed_doc_id.append(i)\n",
    "    \n",
    "    print(f\"Created embeddings for {len(embeddings)} docs and fail to create {failed_doc_counter} embeddings\")\n",
    "            \n",
    "    return embeddings, failed_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f73c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_embeddings(data):\n",
    "    print('Coming soon...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cf7a569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 2417 docs and fail to create 83 embeddings\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-da9ba5c57727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0memb\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'bert'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# Test input embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_test_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bert_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmanual_tagged_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Train input embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Create manual tags matrix for testing\n",
    "y_test = np.zeros((len(manual_tags), 5))\n",
    "\n",
    "for i, file_name in enumerate(manual_tagged_file_names):\n",
    "    for j, tag in enumerate(list(manual_tags[file_name].values())):\n",
    "        if tag:\n",
    "            y_test[i][j] = 1\n",
    "            \n",
    "# Create enhanced tags matrix for training\n",
    "y_aux = np.zeros((len(enhanced_tagged_data) + len(extended_data), 5))\n",
    "\n",
    "for i, file_name in enumerate(enhanced_tagged_file_names):\n",
    "    for j, tag in enumerate(list(enhanced_tags[file_name].values())):\n",
    "        if tag:\n",
    "            y_aux[i][j] = 1\n",
    "\n",
    "# Add tag to last position on array if not tagged\n",
    "for i, y in enumerate(y_test):\n",
    "    if not sum(y):\n",
    "        y[-1] = 1\n",
    "\n",
    "for i, y in enumerate(y_aux):\n",
    "    if not sum(y):\n",
    "        y[-1] = 1\n",
    "\n",
    "# Get Embeddings \n",
    "if emb == 'bert':\n",
    "    # Test input embeddings\n",
    "    X_test, f_test_id = get_bert_embedding(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings, f_enhanced_id = get_bert_embedding(enhanced_tagged_data)\n",
    "    extended_embeddings, f_extended_id = get_bert_embedding(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings\n",
    "    \n",
    "elif emb == 'doc2vec':\n",
    "    # Test input embeddings\n",
    "    X_test = get_doc2vec_embedding(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings = get_doc2vec_embedding(enhanced_tagged_data)\n",
    "    extended_embeddings = get_doc2vec_embedding(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings\n",
    "\n",
    "# Remove failed docs\n",
    "failed_doc_id = f_enhanced_id + f_extended_id\n",
    "\n",
    "y_test = np.delete(y_test, f_test_id)\n",
    "y_train = np.delete(y_train, failed_doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44430d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5c018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "# Input\n",
    "np.save(f'X_train_{emb}_{lang}.npy', X_train)\n",
    "np.save(f'X_val_{emb}_{lang}.npy', X_val)\n",
    "np.save(f'X_test_{emb}_{lang}.npy', X_test)\n",
    "# Tags\n",
    "np.save(f'y_train_{emb}_{lang}.npy', y_train)\n",
    "np.save(f'y_val_{emb}_{lang}.npy', y_val)\n",
    "np.save(f'y_test_{emb}_{lang}.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
