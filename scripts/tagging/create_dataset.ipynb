{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e655e4f8",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b47f0",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ea3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "import torch, re\n",
    "\n",
    "# Display\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "lang = 'en'\n",
    "emb = 'bert'\n",
    "tag_type = 'keywords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a603cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils\n",
    "\n",
    "# Instanciate utils class\n",
    "utils = Utils(r'D:\\Cesard\\Documents\\NLP', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5afadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load manual tags\n",
    "with open(f'tags/manual/reddit_{lang}.json', 'r+') as file_str:\n",
    "    reddit_manual_tags = json.load(file_str)\n",
    "with open(f'tags/manual/tweets_{lang}.json', 'r+') as file_str:\n",
    "    twitter_manual_tags = json.load(file_str)\n",
    "\n",
    "# Load keyword tags\n",
    "with open(f'tags/{tag_type}/reddit_{lang}_words.json', 'r+') as file_str:\n",
    "    reddit_enhanced_tags = json.load(file_str)\n",
    "with open(f'tags/{tag_type}/tweets_{lang}_words.json', 'r+') as file_str:\n",
    "    twitter_enhanced_tags = json.load(file_str)\n",
    "\n",
    "# Load Tagged data\n",
    "print('Starting to load manual tagged data...')\n",
    "manual_reddit_data, manual_reddit_file_names = utils.tagged_data_loader(list(reddit_manual_tags.keys()), 'reddit', lang)\n",
    "manual_twitter_data, manual_twitter_file_names = utils.tagged_data_loader(list(twitter_manual_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(manual_twitter_data)} tagged Tweets {len(manual_reddit_data)} and tagged Reddit docs')\n",
    "print('')\n",
    "\n",
    "# Load Enhanced Tagged data\n",
    "print('Starting to load keyword tagged data...')\n",
    "enhanced_reddit_data, enhanced_reddit_file_names = utils.tagged_data_loader(list(reddit_enhanced_tags.keys()), 'reddit', lang)\n",
    "enhanced_twitter_data, enhanced_twitter_file_names = utils.tagged_data_loader(list(twitter_enhanced_tags.keys()), 'tweets', lang)\n",
    "print(f'Loaded {len(enhanced_twitter_data)} tagged Tweets {len(enhanced_reddit_data)} and tagged Reddit docs')\n",
    "\n",
    "# Load Not Tagged data\n",
    "print('Starting to load not tagged data...')\n",
    "reddit_data, _, _ = utils.data_loader(lang, 'reddit', total_data=20000, max_size = None)\n",
    "twitter_data, _, _ = utils.data_loader(lang, 'tweets', total_data=20000, max_size = None)\n",
    "print(f'Loaded {len(reddit_data)} tagged Tweets {len(twitter_data)} and tagged Reddit docs')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a367412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lists\n",
    "manual_tags = {**twitter_manual_tags , **reddit_manual_tags}\n",
    "manual_tagged_data = manual_twitter_data + manual_reddit_data \n",
    "manual_tagged_file_names = manual_twitter_file_names + manual_reddit_file_names \n",
    "\n",
    "enhanced_tags = {**twitter_enhanced_tags , **reddit_enhanced_tags}\n",
    "enhanced_tagged_data = enhanced_twitter_data + enhanced_reddit_data\n",
    "enhanced_tagged_file_names = enhanced_twitter_file_names + enhanced_reddit_file_names\n",
    "\n",
    "extended_data = set(twitter_data + reddit_data) - set(manual_tagged_data + enhanced_tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111b505",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c9d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_social(data, language='en'):\n",
    "    \n",
    "    # Creates the language dictionary\n",
    "    lang_dict = {\n",
    "        \"en\": \"english\",\n",
    "        \"es\": \"spanish\",\n",
    "        \"fr\": \"french\"\n",
    "    }\n",
    "    \n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    # Sets text into lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Tokenizes by word\n",
    "    tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    data = tk.tokenize(data)\n",
    "    \n",
    "    data_temp = []\n",
    "    for word in data:\n",
    "        if word not in string.punctuation:\n",
    "            data_temp.append(word)\n",
    "    data = data_temp\n",
    "    \n",
    "    # Removes stopwords\n",
    "    data = [token for token in data if token not in stopwords.words(lang_dict[language])]\n",
    "    \n",
    "    # Creates the stemmer\n",
    "    stemmer = SnowballStemmer(lang_dict[language])\n",
    "    \n",
    "    # Stems data\n",
    "    data = [stemmer.stem(token) for token in data]\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be40f31",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "model_name = \"microsoft/xtremedistil-l6-h384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=False)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(data):\n",
    "    \n",
    "    # Preprocess data\n",
    "    corpus = []\n",
    "    for d in data:\n",
    "        corpus.append(preprocess_social, language=lang)\n",
    "        \n",
    "    # Array to save embeddings\n",
    "    embeddings = []\n",
    "    failed_doc_counter = 0\n",
    "    for i, doc in enumerate(corpus):\n",
    "        try:\n",
    "            # Run Bert for each document\n",
    "            inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "            inputs.to(device)\n",
    "            outputs = model(**inputs)\n",
    "            # CLS Token Output\n",
    "            embedding = outputs['pooler_output'].detach().cpu().numpy()[0]\n",
    "            # Append representation\n",
    "            embeddings.append(embedding)\n",
    "        except:\n",
    "            failed_doc_counter += 1\n",
    "    \n",
    "    print(f\"Created embeddings for {len(embeddings)} docs and fail to create {failed_doc_counter} embeddings\")\n",
    "            \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e07e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc2vec_embeddings(data):\n",
    "    print('Coming soon...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981d87d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manual tags matrix for testing\n",
    "y_test = np.zeros((len(manual_tags), 5))\n",
    "\n",
    "for i, file_name in enumerate(manual_tagged_file_names):\n",
    "    for j, tag in enumerate(list(manual_tags[file_name].values())):\n",
    "        if tag:\n",
    "            y_test[i][j] = 1\n",
    "            \n",
    "# Create enhanced tags matrix for training\n",
    "y_aux = np.zeros((len(enhanced_tagged_data) + len(extended_data), 5))\n",
    "\n",
    "for i, file_name in enumerate(enhanced_tagged_file_names):\n",
    "    for j, tag in enumerate(list(enhanced_tags[file_name].values())):\n",
    "        if tag:\n",
    "            y_aux[i][j] = 1\n",
    "\n",
    "# Add tag to last position on array if not tagged\n",
    "for i, y in enumerate(y_test):\n",
    "    if not sum(y):\n",
    "        y[i][-1] = 1\n",
    "\n",
    "for i, y in enumerate(y_aux):\n",
    "    if not sum(y):\n",
    "        y[i][-1] = 1\n",
    "\n",
    "# Get Embeddings \n",
    "if emb == 'bert':\n",
    "    # Test input embeddings\n",
    "    X_test = get_bert_embedding(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings = get_bert_embedding(enhanced_tagged_data)\n",
    "    extended_embeddings = get_bert_embedding(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings\n",
    "    \n",
    "elif emb == 'doc2vec':\n",
    "    # Test input embeddings\n",
    "    X_test = get_doc2vec_embedding(manual_tagged_data)\n",
    "    \n",
    "    # Train input embeddings\n",
    "    enhanced_embeddings = get_doc2vec_embedding(enhanced_tagged_data)\n",
    "    extended_embeddings = get_doc2vec_embedding(extended_data)\n",
    "    X_aux = enhanced_embeddings + extended_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46be566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aux, y_aux, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e952092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "\n",
    "# Input\n",
    "np.save(f'X_train_{emb}_{lang}.npy', X_train)\n",
    "np.save(f'X_val_{emb}_{lang}.npy', X_val)\n",
    "np.save(f'X_test_{emb}_{lang}.npy', X_test)\n",
    "# Tags\n",
    "np.save(f'y_train_{emb}_{lang}.npy', y_train)\n",
    "np.save(f'y_val_{emb}_{lang}.npy', y_val)\n",
    "np.save(f'y_test_{emb}_{lang}.npy', y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
