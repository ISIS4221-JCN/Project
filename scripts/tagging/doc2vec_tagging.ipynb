{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Tagged Data with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\CESAR\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Import Gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate utils class\n",
    "utils = Utils(r'D:\\Cesard\\Documents\\NLP', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load tagged data...\n",
      "Starting 10 threads to load 1500 documents from reddit in es\n",
      "Loaded 1500 files in 0.19 seconds.\n",
      "Starting 10 threads to load 1000 documents from tweets in es\n",
      "Loaded 1000 files in 0.11 seconds.\n",
      "Loaded 1000 tagged Tweets 1500 and tagged Reddit docs\n",
      "\n",
      "Starting to load not tagged data...\n",
      "Starting 10 threads to load 9113 documents from reddit in es\n",
      "Loaded 8437 files in 1.41 seconds.\n",
      "Removed 676 files becasuse they were too large\n",
      "Starting 10 threads to load 241995 documents from tweets in es\n",
      "Loaded 241994 files in 192.68 seconds.\n",
      "Removed 1 files becasuse they were too large\n",
      "Loaded 241994 Tweets 8437 and Reddit docs whithout tag\n"
     ]
    }
   ],
   "source": [
    "# Define language\n",
    "lang = 'es'\n",
    "\n",
    "# Load tags\n",
    "with open(f'tags/tweets_{lang}.json', 'r+') as file_str:\n",
    "    twitter_tags = json.load(file_str)\n",
    "    \n",
    "with open(f'tags/reddit_{lang}.json', 'r+') as file_str:\n",
    "    reddit_tags = json.load(file_str)\n",
    "\n",
    "\n",
    "# Load Tagged data\n",
    "print('Starting to load tagged data...')\n",
    "\n",
    "tagged_reddit_data, tagged_reddit_file_names = utils.tagged_data_loader(list(reddit_tags.keys()), 'reddit', lang)\n",
    "tagged_twitter_data, tagged_twitter_file_names = utils.tagged_data_loader(list(twitter_tags.keys()), 'tweets', lang)\n",
    "\n",
    "print(f'Loaded {len(tagged_twitter_data)} tagged Tweets {len(tagged_reddit_data)} and tagged Reddit docs')\n",
    "\n",
    "# Load Not tagged data\n",
    "print('')\n",
    "print('Starting to load not tagged data...')\n",
    "\n",
    "reddit_data, _ = utils.data_loader(lang, 'reddit', total_data=None, max_size = 300, return_dates = False)\n",
    "twitter_data, _ = utils.data_loader(lang, 'tweets', total_data=None, max_size = 300, return_dates = False)\n",
    "\n",
    "print(f'Loaded {len(twitter_data)} Tweets {len(reddit_data)} and Reddit docs whithout tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lists\n",
    "data = twitter_data + reddit_data\n",
    "\n",
    "tags = {**twitter_tags , **reddit_tags}\n",
    "tagged_data = tagged_twitter_data + tagged_reddit_data\n",
    "tagged_file_names = tagged_twitter_file_names + tagged_reddit_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Ideal Docs\n",
    "vaccines = ['inicia vacunación', 'vacunas pfizer', 'vacunas aztraseneca']\n",
    "mental_health = ['estrés, ansiedad y problemas psicologicos asociados a la salud mental']\n",
    "school_reopening = ['cierre y reapertura de colegios o instituciones educativas']\n",
    "household_violence = ['']\n",
    "\n",
    "ideal_docs = vaccines + mental_health + school_reopening + household_violence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Stemmers\n",
    "stem = SnowballStemmer('english')\n",
    "#p_stem = PorterStemmer()\n",
    "\n",
    "# Tokenizers\n",
    "tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "#tk = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Lemmatizer\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Preprocess tagged data\n",
    "processed_data = []\n",
    "for d in tagged_data:\n",
    "    processed_data.append(utils.preprocessing(d, stop_words = stop_words,\n",
    "                                                 stemmer = None,\n",
    "                                                 tokenizer = tk,\n",
    "                                                 lemmatizer = lemma))\n",
    "\n",
    "# Preprocess data without tag\n",
    "for d in data:\n",
    "    processed_data.append(utils.preprocessing(d, stop_words = stop_words,\n",
    "                                                 stemmer = None,\n",
    "                                                 tokenizer = tk,\n",
    "                                                 lemmatizer = lemma))\n",
    "# Preprocess data without tag\n",
    "for d in ideal_docs:\n",
    "    processed_data.append(utils.preprocessing(d, stop_words = stop_words,\n",
    "                                                 stemmer = None,\n",
    "                                                 tokenizer = tk,\n",
    "                                                 lemmatizer = lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group most common bigrams\n",
    "sent = [row for row in processed_data]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "corpus = bigram[sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "def train_doc2vec(string_data, max_epochs, vec_size, alpha):\n",
    "    \n",
    "    # Tagging each of the data with an ID, and I use the most memory efficient one of just using it's ID\n",
    "    tagged_data = [TaggedDocument(words=d, tags=[str(i)]) for i, d in enumerate(string_data)]\n",
    "    \n",
    "    # Instantiating my model\n",
    "    model = Doc2Vec(alpha=alpha, min_alpha=0.00025, min_count=10, dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data, total_examples = model.corpus_count, epochs=model.epochs)\n",
    "        # Decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # Fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    # Saving model\n",
    "    model.save(\"en_d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "# Training\n",
    "train_doc2vec(corpus, max_epochs = 25, vec_size = 20, alpha = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tags matrix\n",
    "tags_matrix = np.zeros((len(tagged_file_names), 5))\n",
    "\n",
    "for i, file_name in enumerate(tagged_file_names):\n",
    "    for j, tag in enumerate(list(tags[file_name].values())):\n",
    "        if tag:\n",
    "            tags_matrix[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in my model\n",
    "model = Doc2Vec.load(\"en_d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(tagged_data)):\n",
    "    if tags_matrix[i][3]:\n",
    "        # Representative Doc (Tagged)\n",
    "        print(f'TAGGED DOC WITH ID {i}')\n",
    "        print((tagged_data + data)[i])\n",
    "        print(processed_data[i])\n",
    "        print()\n",
    "        \n",
    "        # Similar Docs\n",
    "        print('TOP SIMILAR DOCS')\n",
    "        similar_docs = model.docvecs.most_similar(str(i), topn = 5)\n",
    "        for docs in similar_docs:\n",
    "            print((tagged_data + data)[int(docs[0])])\n",
    "            #print(processed_data[int(docs[0])])\n",
    "        print('--------------------------------------------------')\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
