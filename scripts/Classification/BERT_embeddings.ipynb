{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fam7B-p-mUl4"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Main\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string, json\n",
    "import torch, re\n",
    "\n",
    "# NLP\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "\n",
    "# Display\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "167RW6STmWWa"
   },
   "outputs": [],
   "source": [
    "def preprocess_social(data, language='en'):\n",
    "    \n",
    "    # Creates the language dictionary\n",
    "    lang_dict = {\n",
    "        \"en\": \"english\",\n",
    "        \"es\": \"spanish\",\n",
    "        \"fr\": \"french\"\n",
    "    }\n",
    "    \n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    \n",
    "    # Sets text into lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Tokenizes by word\n",
    "    tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    data = tk.tokenize(data)\n",
    "    \n",
    "    data_temp = []\n",
    "    for word in data:\n",
    "        if word not in string.punctuation:\n",
    "            data_temp.append(word)\n",
    "    data = data_temp\n",
    "    \n",
    "    # Removes stopwords\n",
    "    data = [token for token in data if token not in stopwords.words(lang_dict[language])]\n",
    "    \n",
    "    # Creates the stemmer\n",
    "    stemmer = SnowballStemmer(lang_dict[language])\n",
    "    \n",
    "    # Stems data\n",
    "    data = [stemmer.stem(token) for token in data]\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bp4LV6WamZB0"
   },
   "outputs": [],
   "source": [
    "def preprocess_news(data, language='en'):\n",
    "    \n",
    "    # Creates the language dictionary\n",
    "    lang_dict = {\n",
    "        \"en\": \"english\",\n",
    "        \"es\": \"spanish\",\n",
    "        \"fr\": \"french\"\n",
    "    }\n",
    "    \n",
    "    # Sets text into lowercase\n",
    "    data = data.lower()\n",
    "    \n",
    "    # Removes punctuation\n",
    "    data = strip_punctuation(data)\n",
    "    \n",
    "    # Tokenizes by word\n",
    "    data = word_tokenize(data)\n",
    "    \n",
    "    # Removes stopwords\n",
    "    data = [token for token in data if token not in stopwords.words(lang_dict[language])]\n",
    "    \n",
    "    # Creates the stemmer\n",
    "    stemmer = SnowballStemmer(lang_dict[language])\n",
    "    \n",
    "    # Stems data\n",
    "    data = [stemmer.stem(token) for token in data]\n",
    "    \n",
    "    # Returns preprocessed text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wd19rVByma2A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/juan/NLP/project/lib/python3.6/site-packages (0.1.95)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/home/juan/NLP/project/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from utils import Utils\n",
    "utils = Utils('/media/juan/Juan/NLP/', num_workers=10)\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ENtmQUtOmegL"
   },
   "outputs": [],
   "source": [
    "# Bert instance EN (COVID)\n",
    "model_name = \"microsoft/xtremedistil-l6-h384-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=False)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Bert instance FR\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "# model = AutoModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\", output_hidden_states=True)\n",
    "\n",
    "# Bert instance ES (BETICO)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", output_hidden_states=False)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-ag-news\", device=0)\n",
    "# model = AutoModel.from_pretrained(\"textattack/bert-base-uncased-ag-news\", output_hidden_states=False)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xZ_TL-8wmh6C"
   },
   "outputs": [],
   "source": [
    "def load_sub_data(sub_data, data_type='social'):\n",
    "    data = []\n",
    "    names = []\n",
    "    for file in sub_data:\n",
    "        with open(os.path.join(path_prefix, source, lang, file)) as data_file:\n",
    "            data_dict = json.load(data_file)\n",
    "            names.append(file)\n",
    "            if data_type == 'social':\n",
    "                data.append(preprocess_social(data_dict['text'], language=lang))\n",
    "            else:\n",
    "                data.append(utils.preprocessing(file, stop_words = stop_words, stemmer = None,\n",
    "                                                        tokenizer = tk,\n",
    "                                                        lemmatizer = lemma))\n",
    "                            # data.append(preprocess_news(data_dict['text'], language=lang))\n",
    "    return data, names\n",
    "\n",
    "def infer_embedding_and_save(sub_data, names, failed):\n",
    "    # Array to save embeddings\n",
    "    reu_embeddings = []\n",
    "    failed_doc_ids = []\n",
    "    embedding_dict = {}\n",
    "    for i, doc in enumerate(sub_data):\n",
    "        # try:\n",
    "            # Run Bert for each document\n",
    "        inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "        inputs.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        # CLS Token Output\n",
    "        embedding = outputs['pooler_output'].detach().cpu().numpy()[0].tolist()\n",
    "        # Append representation\n",
    "        reu_embeddings.append(embedding)\n",
    "        # except:\n",
    "        #     failed_doc_ids.append(i)\n",
    "            \n",
    "    # Remove failed docs\n",
    "    for i, doc_id in enumerate(failed_doc_ids):\n",
    "        sub_data.pop(doc_id - i)\n",
    "        names.pop(doc_id - i)\n",
    "    failed = failed + len(failed_doc_ids)\n",
    "    # print(f'Failed to tokenize {len(failed_doc_ids)} documents')\n",
    "    \n",
    "    for file in zip(names, reu_embeddings):\n",
    "        embedding_dict[file[0]] = file[1]\n",
    "    \n",
    "    return embedding_dict, failed\n",
    "\n",
    "def load_and_save_file(embedding_dict, counter):\n",
    "    with open(os.path.join(path_prefix, 'embeddings', source, lang, 'BERT_' + str(counter) + '.json'), 'w') as file:\n",
    "        json.dump(embedding_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_Xq0WfbFmkOI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing tweets in en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:01:35<00:00,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to tokenize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_prefix = '/media/juan/Juan/NLP/'\n",
    "langs = ['en']\n",
    "sources = ['tweets']\n",
    "for lang in langs:\n",
    "    for source in sources:\n",
    "        # Stop Words\n",
    "        lang_dict = {\n",
    "                \"en\": \"english\",\n",
    "                \"es\": \"spanish\",\n",
    "                \"fr\": \"french\"\n",
    "            }\n",
    "        stop_words = stopwords.words(lang_dict[lang])\n",
    "        # Stemmers\n",
    "        stem = SnowballStemmer(lang_dict[lang])\n",
    "        #p_stem = PorterStemmer()\n",
    "        # Tokenizers\n",
    "        #tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "        tk = nltk.RegexpTokenizer(r'\\w+')\n",
    "        # Lemmatizer\n",
    "        lemma = nltk.stem.WordNetLemmatizer()\n",
    "        failed = 0\n",
    "        print('Processing ' + source + ' in ' + lang)\n",
    "        files_list = os.listdir(os.path.join(path_prefix, source, lang))\n",
    "        print(len(files_list))\n",
    "        data_type = 'news'\n",
    "        file_name = source + '_' + lang + 'embeddings.json'\n",
    "        splitted_files_list = np.array_split(files_list, 1000)\n",
    "        for i, chunk in enumerate(tqdm(splitted_files_list)):\n",
    "            data, names = load_sub_data(chunk, data_type = data_type)\n",
    "            for index in range(len(data)):\n",
    "                data[index] = data[index][:450]\n",
    "            embedding_dict, failed = infer_embedding_and_save(data, names, failed)\n",
    "            load_and_save_file(embedding_dict, i)\n",
    "        print('Failed to tokenize ' + str(failed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
