{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878205c1-5f9e-4e5a-b360-f9d19668576c",
   "metadata": {},
   "source": [
    "# Fake News Detection - English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed2e4b9-e329-4a59-a033-0a10cc95bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Main\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel, TFAutoModel\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Display\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49172c9e-a4a6-412f-a170-f61f58b5b99c",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ad31e9-f933-44e8-8327-57f2235a5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Utils\n",
    "utils = Utils('/media/juan/Juan/NLP/', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e6234-83be-49ad-9eb4-1221e2423950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Starting 10 threads to load 10000 documents from news in en\n",
      "Loaded 10000 files in 301.18 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 10000 documents from FakeNews in en\n"
     ]
    }
   ],
   "source": [
    "# Define language\n",
    "lang = 'en'\n",
    "\n",
    "print('Starting...')\n",
    "\n",
    "news_data, _ = utils.data_loader(lang, 'news', total_data=10000, max_size = None, return_dates = False)\n",
    "fake_news_data, _ = utils.data_loader(lang, 'FakeNews', total_data=10000, max_size = None, return_dates = False)\n",
    "\n",
    "print(f'Loaded {len(news_data)} news {len(fake_news_data)} fake news')\n",
    "\n",
    "data = news_data + fake_news_data\n",
    "tags = [1]*len(news_data) + [0]*len(fake_news_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd198a-3691-402e-ba0a-9a7f6ca03523",
   "metadata": {},
   "source": [
    "### Glimpse at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca6182-58d1-4770-a090-d8104de70fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_num = 5\n",
    "print(news_data[news_num])\n",
    "print('-----------------------')\n",
    "print(fake_news_data[news_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74b64c0-22e1-4fc6-ad37-b5689b4d8cf4",
   "metadata": {},
   "source": [
    "### Feature extraction using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3750d925-0429-4f91-a818-9717db193434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:27<00:00, 363.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "# Stemmers\n",
    "stem = SnowballStemmer('english')\n",
    "#p_stem = PorterStemmer()\n",
    "# Tokenizers\n",
    "#tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "tk = nltk.RegexpTokenizer(r'\\w+')\n",
    "# Lemmatizer\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "# Preprocess data\n",
    "corpus = []\n",
    "for d in tqdm(data):\n",
    "    corpus.append(utils.preprocessing(d, stop_words = stop_words,\n",
    "                                         stemmer = None,\n",
    "                                         tokenizer = tk,\n",
    "                                         lemmatizer = lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64c48a-9cb6-4c7b-8725-e3372ca67e59",
   "metadata": {},
   "source": [
    "####  Reduce data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e605f246-1e02-4a1b-8a1e-e997290ff07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f28a8d-306e-4a85-9d66-95480d1e602d",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be3a506-a3b9-4733-b928-3391069e99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert instance EN (COVID)\n",
    "model_name = \"mrm8488/distilroberta-finetuned-age_news-classification\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModel.from_pretrained(model_name, output_hidden_states=False)\n",
    "\n",
    "# Bert instance FR\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "# model = AutoModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\", output_hidden_states=True)\n",
    "\n",
    "# Bert instance ES (BETICO)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\", output_hidden_states=False)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-ag-news\", device=0)\n",
    "# model = AutoModel.from_pretrained(\"textattack/bert-base-uncased-ag-news\", output_hidden_states=False)mrm8488/bert-mini-finetuned-age_news-classification\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e880793-313d-4412-a8cc-7294384425fc",
   "metadata": {},
   "source": [
    "#### Obtain first doc's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801151b0-f929-40f1-a529-c38cd77f273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model for first sentence\n",
    "inputs = tokenizer(corpus[0], return_tensors=\"pt\", is_split_into_words=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Just pooler output as embeddings\n",
    "embedding = outputs['pooler_output'].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80e5f6-45da-455e-9996-1e9bd930439d",
   "metadata": {},
   "source": [
    "#### Tokenize each doch in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546bb653-c5c5-4ebb-adbe-c5176659dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [16:41<00:00,  9.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to tokenize 0 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Array to save embeddings\n",
    "reu_embeddings = []\n",
    "\n",
    "failed_doc_ids = []\n",
    "\n",
    "for i, doc in enumerate(tqdm(corpus)):\n",
    "    try:\n",
    "        # Run Bert for each document\n",
    "        inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # CLS Token Output\n",
    "        embedding = outputs['pooler_output'].detach().numpy()[0]\n",
    "        \n",
    "        # Append representation\n",
    "        reu_embeddings.append(embedding)\n",
    "        \n",
    "    except:\n",
    "        failed_doc_ids.append(i)\n",
    "    \n",
    "print(f'Failed to tokenize {len(failed_doc_ids)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a95ea-66d9-4631-b3ad-71b7f34a7515",
   "metadata": {},
   "source": [
    "#### Remove failed documents and corresponding tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c24e91c-bdd9-4f77-9daa-c5ca069392fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove failed docs\n",
    "for i, doc_id in enumerate(failed_doc_ids):\n",
    "    corpus.pop(doc_id - i)\n",
    "    tags.pop(doc_id - i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474f7ee-6dcb-4154-88e3-cf5fbf756bde",
   "metadata": {},
   "source": [
    "#### Split data and train MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203b7b7-3451-40e4-9195-f0a92fd89623",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'NB': GaussianNB(), \n",
    "              'LR': LogisticRegression(random_state=0, max_iter = 700), \n",
    "              'MLP': MLPClassifier(hidden_layer_sizes=(500,250,100,20), random_state=1, max_iter=700),\n",
    "              'SVM': svm.SVC()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8bb7f-70f2-4e76-a762-61c0ef10f171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sklearn_models(model, metrics_df, X, y, vectorizer_type, val_percentage = 0.2):\n",
    "    \"\"\" Train and evaluate model with specified arguments\n",
    "    \n",
    "    Args:\n",
    "        model (str): Model to train\n",
    "        metrics_df (pd.DataFrame): Dataframe to save the results\n",
    "        X (np.ndarray): Features of data\n",
    "        y (list): Tags of X\n",
    "        vectorizer_type (str): Vectorizer used to extract features\n",
    "        val_percentage (float): Validation percentage\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Results of trained and evaluated model\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=1-val_percentage)\n",
    "    clf = model_dict[model]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    pre = precision_score(y_val, y_pred)\n",
    "    rec = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    data = {'model': [model], 'features': [vectorizer_type], 'lang': [lang], 'accuracy': [acc], 'precision': [pre], 'recall': [rec], 'f1': [f1]}\n",
    "    df = metrics_df.append([pd.DataFrame(data=data)])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb388fe-1f6b-4fe4-b3b7-b59290056311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "        layers.Dense(500, activation=\"relu\", input_shape=(len(X_train), 768)),\n",
    "        layers.Dense(250, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(np.array(X_train), np.array(y_train), validation_data=(np.array(X_val), np.array(y_val)), epochs=300)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd4a97-1fb2-4551-89f0-d13042b9b34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe910ea-e068-48f1-86f4-de1897ae598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c715df8-06d5-47ce-98da-7cee691e3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,250,100,20), random_state=1, max_iter=700).fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b886933-a456-4b97-a33e-a66d0899b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba77d38-0adc-481e-b2ab-ed1be962fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, max_iter = 700).fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44560f3a-1985-4656-a57b-75a3869bd7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
