{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a8de76-8a5d-42c9-af76-5850907bae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# Main\n",
    "import sys\n",
    "sys.path.insert(0,'../')\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0ad31e9-f933-44e8-8327-57f2235a5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Utils\n",
    "utils = Utils('/media/juan/Juan/NLP/', num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33e6234-83be-49ad-9eb4-1221e2423950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Starting 10 threads to load 10000 documents from news in es\n",
      "Loaded 10000 files in 256.46 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Starting 10 threads to load 10000 documents from FakeNews in es\n",
      "Loaded 10000 files in 20.23 seconds.\n",
      "Removed 0 files becasuse they were too large\n",
      "Loaded 10000 Tweets 10000 Reddit docs\n"
     ]
    }
   ],
   "source": [
    "# Define language\n",
    "lang = 'es'\n",
    "\n",
    "print('Starting...')\n",
    "\n",
    "news_data, _ = utils.data_loader(lang, 'news', total_data=10000, max_size = None, return_dates = False)\n",
    "fake_news_data, _ = utils.data_loader(lang, 'FakeNews', total_data=10000, max_size = None, return_dates = False)\n",
    "\n",
    "print(f'Loaded {len(news_data)} Tweets {len(fake_news_data)} Reddit docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bca6182-58d1-4770-a090-d8104de70fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                    \n",
      "                    \n",
      "                       S24H   \n",
      "   \n",
      "   \n",
      "            \n",
      "                \n",
      "                    12/05/2021 10:02\n",
      "                \n",
      "            \n",
      "                \n",
      "                    \n",
      "                        \n",
      "                        \n",
      "                      Comentarios\n",
      "                \n",
      "            Inserta Empleo, a través de la  Asociación Salud Mental Salamanca AFEMC, ha adjudicado 3  acciones formativas dirigidas al colectivo de personas con discapacidad.  En esta ocasión los títulos de estas formaciones son: “Planificación y  Organización 15h”, “Atención básica al cliente 50h” y “Motivación  30h”.  Un grupo integrado por 30 personas participarán en estas  formaciones, donde podrán adquirir habilidades enfocadas a la mejora de  la empleabilidad. Los cursos se enmarcan en el Programa Operativo de Inclusión Social y  Economía Social (POISES) y Programa Operativo de Empleo Juvenil  (POEJ), que está desarrollando Fundación ONCE a través de Inserta  Empleo, con la cofinanciación del Fondo Social Europeo, con el  objetivo de incrementar la formación y el empleo de las personas con  discapacidad.  Este tipo de acciones están dirigidas a mejorar la inserción laboral en  un colectivo de población que presenta las tasas más bajas de  acceso al empleo.  Para incrementar esta empleabilidad, desde Inserta se plantean  acciones formativas de este tipo en las que el objetivo es potenciar la  mejora en habilidades y de cualificación de las personas con  discapacidad, de tal forma que se aumenten sus posibilidades de acceso  al mercado laboral. \n",
      "Bodas: María Juncadella y Carlota Redón: dos despedidas de soltera para las novias de la jet - Vanitatis. \n",
      "En el Museo del Claustro de Mujeres Votadas se encuentran las siguientes colecciones: Los siete abrigos originales femeninos de la Compañía de las Marinas: Carmen Leñero, Carmen Leñero, María Juncadella, Carlota Redón, María José Luis, María Manuela, Emilia Paire, María José de Paula y Luisa de la Concha, Carmen. En la primera colección de mujeres no rusas \"La Familia de la Juventud\", se han encontrado unos trece abrieron en París, la primera de ellas en 1917 (dos de los cuales fueron abiertos con motivo de la celebración del Día Internacional de las Mujeres), fue una auténtica museo de historia y cultura rusa en cuanto a lo que se convirtió en la sociedad rusa en su época y es un buen ejemplo del poder de crear valor y belleza en la sociedad rusa. Y un trabajo que representa, por ejemplo, el surgimiento de dos países, como la Unión Soviética y la República Popular de China.\n",
      "\n",
      "Las novias rusas de Mujeres Votadas son muy conocidas después de ser abiertas al público en París con el nombre de Mujeres Votadas, pero nunca se abrirá a ellos al público en Moscú. Muchas de estas novias están activas y participan en eventos para sus conciencias. Su vida también es reconocida en otras partes del mundo, especialmente en Estados Unidos y más recientemente en Francia, y está disponible en muchos museos rusos.\n",
      "\n",
      "-----------------------\n",
      "PRECIO OBJETIVO: PRECIO ACTUAL: RECOMENDACIÓN: CORTO PLAZO:MEDIO PLAZO:LARGO PLAZO:Nota: Datos de cotización y análisis actualizados a día de hoy.Rovi, la farmacéutica madrileña de la familia López-Belmonte, sube más del 4% en Bolsa tras ampliar su acuerdo con la estadounidense Moderna. Ya no solo se limitará al envasado y empaquetado de la vacuna contra el coronavirus, sino que también participará en la fabricación del principio activo y en su formulación.Rovi ha reforzado el acuerdo que firmó con la biofarmacéutica estadounidense Moderna el pasado mes de julio y ya no solo se limitará al acabado de la vacuna sino que \"participará en la fabricación del principio activo así como en su formulación\", segPara seguir leyendo hágase Premium1€ / primer mesy disfrute de acceso ilimitado a todo el contenido web de Expansión\n",
      "¿Ya es Premium? Inicie sesión Cancele cuando quiera   Consulte los términos y condiciones del servicio Santander espera triplicar los ingresos de su plataforma de pagosAlpesa amplía su actividad y espera facturar 30 millones este añoLa 'ruta Magallanes-Elcano' alía a 5 autonomías en una oferta turísticaAragonès se compromete a culminar la independencia en el debate de investiduraMorgan Stanley abre la carrera para suceder a Gorman©  2021 Unidad Editorial Información Económica S.L. Apúntate a nuestras newslettersApuntarmeSíguenos en\n",
      "El Ministerio de Salud reportó que ya hay más de 100.000 casos activos de covid-19 - La República. \n",
      "En la Ciudad de Guatemala, el número de matrimonios entre personas del mismo sexo ha aumentado rápidamente ha sido de 30 a 40. El 18 de octubre de 2011 existen 1.559 nacimientos entre personas del mismo sexo en esta ciudad, es decir la mayoría de estos jóvenes se encuentran en departamentos, regiones y ciudades bajo las nuevas reglas del matrimonio, las personas del mismo sexo que se han unido en matrimonio deben hacer frente a las instituciones, pero también deberán llevar un calendario de vida más grande y la importancia de mantener lazos familiares e intercambiar experiencias a través de la comunidad para evitar el abuso y abusos por parte de las personas con las mismas (también se incluye la comunidad familiar).\n",
      "\n",
      "En este caso la Organización de la Naciones Unidas para la Agricultura, en un principio había estimado que existían más de 20.000 casos en la República de Guatemala en 2011, y las estadísticas oficiales de UNONA para el 2011 estimaron que en un principio fueron en 12,600. \n",
      "\n",
      "En 2009, hubo 3,66.589 nacimientos por mujer; ese mismo año las estadísticas oficiales de CONADEX para el 2011 estimaron que en 10.000 nacimientos hubo 8.200 mujeres y en los 3.000 nacimientos hubo 1,066.675 mujeres y en el 2.200 nacimientos hubo 1,065.035 mujeres y en el 3.200 nacimientos hubo 1,064\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(news_data[i])\n",
    "    print(fake_news_data[i])\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "016a639f-cf70-43f7-a052-7d1c6899a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = news_data + fake_news_data\n",
    "tags = [1]*len(news_data) + [0]*len(fake_news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3750d925-0429-4f91-a818-9717db193434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [01:03<00:00, 313.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Stemmers\n",
    "stem = SnowballStemmer('english')\n",
    "#p_stem = PorterStemmer()\n",
    "\n",
    "# Tokenizers\n",
    "#tk = nltk.tokenize.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "tk = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Lemmatizer\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Preprocess data\n",
    "corpus = []\n",
    "for d in tqdm(data):\n",
    "    corpus.append(utils.preprocessing(d, stop_words = stop_words,\n",
    "                                         stemmer = None,\n",
    "                                         tokenizer = tk,\n",
    "                                         lemmatizer = lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e605f246-1e02-4a1b-8a1e-e997290ff07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i][:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3be3a506-a3b9-4733-b928-3391069e99ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at textattack/bert-base-uncased-ag-news were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/bert-base-uncased-ag-news\", device=0)\n",
    "model = AutoModel.from_pretrained(\"textattack/bert-base-uncased-ag-news\", output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801151b0-f929-40f1-a529-c38cd77f273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Model for first sentence\n",
    "inputs = tokenizer(corpus[0][:259], return_tensors=\"pt\", is_split_into_words=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Just pooler output as embeddings\n",
    "embedding = outputs['pooler_output'].detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bb653-c5c5-4ebb-adbe-c5176659dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4147/20000 [15:14<56:42,  4.66it/s]  "
     ]
    }
   ],
   "source": [
    "# Array to save embeddings\n",
    "reu_embeddings = []\n",
    "\n",
    "failed_doc_ids = []\n",
    "\n",
    "for i, doc in enumerate(tqdm(corpus)):\n",
    "    try:\n",
    "        # Run Bert for each document\n",
    "        inputs = tokenizer(doc, return_tensors=\"pt\", is_split_into_words=True)\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # CLS Token Output\n",
    "        embedding = outputs['pooler_output'].detach().numpy()[0]\n",
    "        \n",
    "        # Append representation\n",
    "        reu_embeddings.append(embedding)\n",
    "        \n",
    "    except:\n",
    "        failed_doc_ids.append(i)\n",
    "    \n",
    "print(f'Failed to tokenize {len(failed_doc_ids)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24e91c-bdd9-4f77-9daa-c5ca069392fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove failed docs\n",
    "for i, doc_id in enumerate(failed_doc_ids):\n",
    "    corpus.pop(doc_id - i)\n",
    "    tags.pop(doc_id - i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a8f8e9-56a3-4fa4-ad66-81cd5817e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reu_embeddings\n",
    "y = tags\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(500,250,100,20), random_state=1, max_iter=700).fit(X_train, y_train)\n",
    "clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb388fe-1f6b-4fe4-b3b7-b59290056311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "        layers.Dense((768)/2, activation=\"relu\", input_shape = X.shape[0]),\n",
    "        layers.Dense((300), activation='relu'),\n",
    "        layers.Dense((100), activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62195d-9dfa-439a-b5c5-360e760a769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
