{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602f57bd-7126-4416-b812-a7517e173096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0b8f2b-82f1-4df5-8aa2-d95a72a2023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate utils class\n",
    "utils = Utils('/media/juan/Juan/NLP/', num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718e7d10-558b-400c-a6cf-402e45a2864a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting threads to load 10000 documents from tweets in en\n",
      "Loaded 10000 files in 14.039345979690552 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load tweets\n",
    "en_tweets = utils.data_loader('en', 'tweets', total_data=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ea5c15-8a5d-4aa3-b8ee-151b086277f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:02<00:00, 4370.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create objects for preprocessing method\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "processed_tweets = []\n",
    "for tweet in tqdm(en_tweets):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    processed_tweets.append(utils.preprocessing(tweet,\n",
    "                            stop_words = stop_words,\n",
    "                            stemmer = stemmer,\n",
    "                            tokenizer = tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0eaedd6-d0d2-4391-ba8b-4cfa04cec0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: When #COVID19 forced them to go remote, schools found creative ways to expand broadband access for students and families. A year later, community leaders say long-term solutions are still needed in Eastern NC. https://t.co/17pwmu479j\n",
      "Processed tweet: ['when', 'covid', '19', 'forced', 'them', 'to', 'go', 'remote', 'schools', 'found', 'creative', 'ways', 'to', 'expand', 'broadband', 'access', 'for', 'students', 'and', 'families', 'year', 'later', 'community', 'leaders', 'say', 'longterm', 'solutions', 'are', 'still', 'needed', 'in', 'eastern', 'nc']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: Duh, President Pudding Brain said we have 800 million vaccines.  That would be more than we need for our population because of math. \n",
      "\n",
      "Send the excess to the open border for goodness sake. \n",
      "\n",
      "States have a new Covid problem: Too much vaccine - POLITICO https://t.co/CcpFlf3jV8\n",
      "Processed tweet: ['duh', 'president', 'pudding', 'brain', 'said', 'we', 'have', '800', 'million', 'vaccines', 'that', 'would', 'be', 'more', 'than', 'we', 'need', 'for', 'our', 'population', 'because', 'of', 'math', 'send', 'the', 'excess', 'to', 'the', 'open', 'border', 'for', 'goodness', 'sake', 'states', 'have', 'new', 'covid', 'problem', 'too', 'much', 'vaccine', 'politico']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: I'll be tweeting the papers when they are done (in June, if they pass), but I will mention a couple highlights now. \n",
      "One paper uses SARIMA models on covid-19 &amp; finds that Gerell et al (2020) may have overstated the impact of covid-19 on violence with their basic method!\n",
      "Processed tweet: ['ill', 'be', 'tweeting', 'the', 'papers', 'when', 'they', 'are', 'done', 'in', 'june', 'if', 'they', 'pass', 'but', 'i', 'will', 'mention', 'couple', 'highlights', 'now', 'one', 'paper', 'uses', 'sarima', 'models', 'on', 'covid', '19', 'amp', 'finds', 'that', 'gerell', 'et', '2020', 'may', 'have', 'overstated', 'the', 'impact', 'of', 'covid', '19', 'on', 'violence', 'with', 'their', 'basic', 'method']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: https://t.co/K2G4OZUQM4 #economics #econ3 labour market issues.\n",
      "Processed tweet: ['economics', 'econ', '3', 'labour', 'market', 'issues']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: Please help... https://t.co/nvl1YBTGm2\n",
      "Processed tweet: ['please', 'help']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in range(5):\n",
    "    rand_num = random.randint(0, len(en_tweets) - 1)\n",
    "    print('Original tweet: ' + en_tweets[rand_num])\n",
    "    print('Processed tweet: ' + str(processed_tweets[rand_num]) + '\\n')\n",
    "    print('-------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bbf5a7f-dade-418b-b3f8-26943c3c18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group most common bigrams\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row for row in processed_tweets]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "tweets = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e3343-00eb-4abb-af52-a0648f54114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n"
     ]
    }
   ],
   "source": [
    "def train_doc2vec(string_data, max_epochs, vec_size, alpha):\n",
    "    # Tagging each of the data with an ID, and I use the most memory efficient one of just using it's ID\n",
    "    tagged_data = [TaggedDocument(words=d, tags=[str(i)]) for i, d in enumerate(string_data)]\n",
    "    \n",
    "    # Instantiating my model\n",
    "    model = Doc2Vec(alpha=alpha, min_alpha=0.00025, min_count=1, dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data, total_examples = model.corpus_count, epochs=model.epochs)\n",
    "        # Decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # Fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    # Saving model\n",
    "    model.save(\"en_d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "# Training\n",
    "train_doc2vec(tweets, max_epochs = 100, vec_size = 20, alpha = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50692bcf-6798-4489-b967-df94e3a2b97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ad2d0-11c8-4157-936f-f6aaac2a8c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
