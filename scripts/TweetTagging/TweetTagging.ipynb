{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "602f57bd-7126-4416-b812-a7517e173096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "# Import utils class\n",
    "sys.path.insert(0,'../')\n",
    "from utils import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0b8f2b-82f1-4df5-8aa2-d95a72a2023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate utils class\n",
    "utils = Utils('/media/juan/Juan/NLP/', num_workers=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718e7d10-558b-400c-a6cf-402e45a2864a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting threads to load 100000 documents from tweets in en\n",
      "Loaded 100000 files in 898.5474643707275 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Load tweets\n",
    "en_tweets = utils.data_loader('en', 'tweets', total_data=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ea5c15-8a5d-4aa3-b8ee-151b086277f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:24<00:00, 4147.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create objects for preprocessing method\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "stop_words = stopwords.words('spanish')\n",
    "\n",
    "processed_tweets = []\n",
    "for tweet in tqdm(en_tweets):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    processed_tweets.append(utils.preprocessing(tweet,\n",
    "                            stop_words = stop_words,\n",
    "                            stemmer = stemmer,\n",
    "                            tokenizer = tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0eaedd6-d0d2-4391-ba8b-4cfa04cec0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: Don’t rehab criminals. A good PR team, a pandemic and a dysfunctional gomen is giving Najib the golden opportunity to wash his slate clean cuz he knws yall dumbass will eat it up. \n",
      "\n",
      "Lest you forget he robbed our nation off billions. And he ain’t even served time yet. https://t.co/d6gG0CH6rN\n",
      "Processed tweet: ['don', '’', 't', 'rehab', 'criminals', 'good', 'pr', 'team', 'pandemic', 'and', 'dysfunctional', 'gomen', 'is', 'giving', 'najib', 'the', 'golden', 'opportunity', 'to', 'wash', 'his', 'slate', 'clean', 'cuz', 'knws', 'yall', 'dumbass', 'will', 'eat', 'it', 'up', 'lest', 'you', 'forget', 'robbed', 'our', 'nation', 'off', 'billions', 'and', 'ain', '’', 't', 'even', 'served', 'time', 'yet']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: @jen_jstephen @drphiliplee1 I can understand your concern. We need a world wide effort to get more vaccine into developing nations to stop more new covid variants coming into being that could spread faster or make more folk seriously ill\n",
      "Processed tweet: ['jenjstephen', 'drphiliplee', '1', 'i', 'can', 'understand', 'your', 'concern', 'we', 'need', 'world', 'wide', 'effort', 'to', 'get', 'more', 'vaccine', 'into', 'developing', 'nations', 'to', 'stop', 'more', 'new', 'covid', 'variants', 'coming', 'into', 'being', 'that', 'could', 'spread', 'faster', 'or', 'make', 'more', 'folk', 'seriously', 'ill']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: Awesome to hear @MayorKimJaney the right move!!! https://t.co/LfgHZmCXmh\n",
      "Processed tweet: ['awesome', 'to', 'hear', 'mayorkimjaney', 'the', 'right', 'move']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: @ICC Sadly corona is coming out on top these days..\n",
      "Processed tweet: ['icc', 'sadly', 'corona', 'is', 'coming', 'out', 'on', 'top', 'these', 'days']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "Original tweet: CrossFit is fucking stupid but she has a great point and I also need to go work on my press https://t.co/s63WbeY2UQ\n",
      "Processed tweet: ['crossfit', 'is', 'fucking', 'stupid', 'but', 'she', 'great', 'point', 'and', 'i', 'also', 'need', 'to', 'go', 'work', 'on', 'my', 'press']\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet in range(5):\n",
    "    rand_num = random.randint(0, len(en_tweets) - 1)\n",
    "    print('Original tweet: ' + en_tweets[rand_num])\n",
    "    print('Processed tweet: ' + str(processed_tweets[rand_num]) + '\\n')\n",
    "    print('-------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7bbf5a7f-dade-418b-b3f8-26943c3c18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group most common bigrams\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row for row in processed_tweets]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
    "bigram = Phraser(phrases)\n",
    "tweets = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e3343-00eb-4abb-af52-a0648f54114d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n"
     ]
    }
   ],
   "source": [
    "def train_doc2vec(string_data, max_epochs, vec_size, alpha):\n",
    "    # Tagging each of the data with an ID, and I use the most memory efficient one of just using it's ID\n",
    "    tagged_data = [TaggedDocument(words=d, tags=[str(i)]) for i, d in enumerate(string_data)]\n",
    "    \n",
    "    # Instantiating my model\n",
    "    model = Doc2Vec(alpha=alpha, min_alpha=0.00025, min_count=1, dm =1)\n",
    "\n",
    "    model.build_vocab(tagged_data)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print('iteration {0}'.format(epoch))\n",
    "        model.train(tagged_data, total_examples = model.corpus_count, epochs=model.epochs)\n",
    "        # Decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # Fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    # Saving model\n",
    "    model.save(\"models/d2v.model\")\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "# Training\n",
    "train_doc2vec(tweets, max_epochs = 100, vec_size = 20, alpha = 0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ad2d0-11c8-4157-936f-f6aaac2a8c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
